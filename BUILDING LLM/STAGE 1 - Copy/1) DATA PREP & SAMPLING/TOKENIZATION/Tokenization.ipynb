{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 1 --> CREATING TOKENS\n",
        "\n"
      ],
      "metadata": {
        "id": "xv2rIyAMHUNG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bT7Guv6mGBtM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "562dcea9-be7f-4888-e727-b815b25d1cfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total No of words 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ],
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  rawText = f.read()\n",
        "print(\"Total No of words\", len(rawText))\n",
        "print(rawText[:99])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "eqlOKFn9HZrd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, world. This, is a test.\"\n",
        "result = re.split(r'(\\s)', text) # Splits wherever whitespace is found\n",
        "print(result)\n",
        "\n",
        "res2 = re.split(r'([,.]|\\s)', text) #Splits wherever comma fullstop and whitespace is found\n",
        "print(res2)\n",
        "\n",
        "res3 = [item for item in res2 if item.strip()] # Whitespaces will not be displayed\n",
        "print(res3)\n",
        "\n",
        "newtext = \"Hello, world!. Is this-- a test ?\"\n",
        "newres = re.split(r'([,.:;?_!\"()\\'] |--|\\s)', newtext)\n",
        "newres = [item.strip() for item in newres if item.strip()] # Splits all unique characters and white spaces will not be displayed\n",
        "print(newres)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3FKMqWuID0m",
        "outputId": "a7b3ce6a-9e70-45a7-8f18-2429317996df"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n",
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n",
            "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n",
            "['Hello', ',', 'world!', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Removing white spaces or not??\n",
        "Removing white spaces reduced the memory and computing requirments but however if the dataset is senditive to the exact structure of the text then dont remove. So it depends upon the requirments of the model\n"
      ],
      "metadata": {
        "id": "CBWF6-NlKYV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preProcessed = re.split(r'([,.:;?_!\"()\\'] |--|\\s)', rawText)\n",
        "preProcessed = [item.strip() for item in preProcessed if item.strip()]\n",
        "print(preProcessed[:100])\n",
        "print(len(preProcessed)) # 4286 tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zm0WL01DKv0_",
        "outputId": "1a309332-3449-48d2-9ba8-1f2394f81246"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself', 'in', 'a', 'villa', 'on', 'the', 'Riviera', '.', '(Though', 'I', 'rather', 'thought', 'it', 'would', 'have', 'been', 'Rome', 'or', 'Florence.)', '\"The', 'height', 'of', 'his', 'glory\"', '--', 'that', 'was', 'what', 'the', 'women', 'called', 'it', '.', 'I', 'can', 'hear', 'Mrs', '.', 'Gideon', 'Thwing', '--', 'his', 'last', 'Chicago', 'sitter', '--', 'deploring', 'his', 'unaccountable', 'abdication', '.']\n",
            "4286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 2 --> Creating Token ID's\n",
        "\n"
      ],
      "metadata": {
        "id": "uBU5lLhPNAE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "allWords = sorted(set(preProcessed)) # Set is used to only sort unique tokens and then unique tokens are sorted in alphabetical order\n",
        "print(allWords[:100])\n",
        "vocabSize = len(allWords)\n",
        "print(vocabSize)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i37ymZfYNFn6",
        "outputId": "998d90f5-4a96-4abc-8787-8ed6c8074277"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['!', '\"', '\"Ah', '\"Be', '\"Begin', '\"By', '\"Come', '\"Destroyed', '\"Don\\'t', '\"Gisburns', '\"Grindles.\"', '\"Hang', '\"Has', '\"How', '\"I', '\"I\\'d', '\"If', '\"It', '\"It\\'s', '\"Jack', '\"Money\\'s', '\"Moon-dancers', '\"Mr', '\"Mrs', '\"My', '\"Never', '\"Never,', '\"Of', '\"Oh', '\"Once', '\"Only', '\"Or', '\"That', '\"The', '\"Then', '\"There', '\"This', '\"We', '\"Well', '\"What', '\"When', '\"Why', '\"Yes', '\"You', '\"but', '\"deadening', '\"dragged', '\"effects\"', '\"interesting\"', '\"lift', '\"obituary', '\"strongest,', '\"strongly', '\"sweetly\"', \"'\", \"'Are\", \"'It's\", \"'coming\", \"'done\", \"'subject.\", \"'technique\", \"'way\", '(I', '(Though', ')', ',', '--', '.', '.\"', ':', ';', '?', 'A', 'Among', 'And', 'Arrt', 'As', 'At', 'Burlington', 'But', 'By', 'Carlo', 'Chicago', 'Claude', 'Croft', 'Devonshire', \"Don't\", 'Dubarry', 'Emperors', 'Florence.)', 'For', 'Gallery', 'Gideon', 'Gisburn', \"Gisburn's\", 'Grafton', 'Greek', 'Grindle', \"Grindle's\", 'Grindle,']\n",
            "1258\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping Using Dictionary Token:Token ID\n",
        "vocabDict = {token:integer for integer, token in enumerate(allWords)}"
      ],
      "metadata": {
        "id": "V3qNpjtlPB0Z"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(vocabDict.items()):\n",
        "  print(item)\n",
        "  if i >= 50:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ip0ZxnhNPW9B",
        "outputId": "3da746bf-a7cc-4b89-a5bb-c17ba4cecf75"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "('\"Ah', 2)\n",
            "('\"Be', 3)\n",
            "('\"Begin', 4)\n",
            "('\"By', 5)\n",
            "('\"Come', 6)\n",
            "('\"Destroyed', 7)\n",
            "('\"Don\\'t', 8)\n",
            "('\"Gisburns', 9)\n",
            "('\"Grindles.\"', 10)\n",
            "('\"Hang', 11)\n",
            "('\"Has', 12)\n",
            "('\"How', 13)\n",
            "('\"I', 14)\n",
            "('\"I\\'d', 15)\n",
            "('\"If', 16)\n",
            "('\"It', 17)\n",
            "('\"It\\'s', 18)\n",
            "('\"Jack', 19)\n",
            "('\"Money\\'s', 20)\n",
            "('\"Moon-dancers', 21)\n",
            "('\"Mr', 22)\n",
            "('\"Mrs', 23)\n",
            "('\"My', 24)\n",
            "('\"Never', 25)\n",
            "('\"Never,', 26)\n",
            "('\"Of', 27)\n",
            "('\"Oh', 28)\n",
            "('\"Once', 29)\n",
            "('\"Only', 30)\n",
            "('\"Or', 31)\n",
            "('\"That', 32)\n",
            "('\"The', 33)\n",
            "('\"Then', 34)\n",
            "('\"There', 35)\n",
            "('\"This', 36)\n",
            "('\"We', 37)\n",
            "('\"Well', 38)\n",
            "('\"What', 39)\n",
            "('\"When', 40)\n",
            "('\"Why', 41)\n",
            "('\"Yes', 42)\n",
            "('\"You', 43)\n",
            "('\"but', 44)\n",
            "('\"deadening', 45)\n",
            "('\"dragged', 46)\n",
            "('\"effects\"', 47)\n",
            "('\"interesting\"', 48)\n",
            "('\"lift', 49)\n",
            "('\"obituary', 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer Class with ENCODE and DECODE\n"
      ],
      "metadata": {
        "id": "3FO3t6e3QAjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self, vocabDict):  # Constructor\n",
        "        self.strToInt = vocabDict  # Encoder dictionary\n",
        "        self.intToStr = {i: s for s, i in vocabDict.items()}  # Decoder dictionary\n",
        "\n",
        "    def encode(self, text):\n",
        "        # Improved tokenization: separate words and punctuation\n",
        "        preProcessed = re.findall(r\"\\w+|[^\\w\\s]\", text)\n",
        "        # Encode tokens; handle unknown tokens if needed\n",
        "        encoded = [self.strToInt[s] for s in preProcessed if s in self.strToInt]\n",
        "        return encoded\n",
        "\n",
        "    def decode(self, encoded):\n",
        "        # Convert token ids back to strings\n",
        "        text = \" \".join([self.intToStr[i] for i in encoded])\n",
        "        # Remove space before punctuation\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text\n",
        "\n",
        "\n",
        "# Previous Class Without Updation\n",
        "# class Tokenizer:\n",
        "#   def __init__(self, vocabDict): # Constructor\n",
        "#     self.strToInt = vocabDict  # Encoder Method\n",
        "#     self.intToStr = {i:s for s, i in vocabDict.items()} # reversing dictionary (Decoder Method)\n",
        "\n",
        "#   def encode(self, text):\n",
        "#     preProcessed = re.split(r'([,.:;?_!\"()\\'] |--|\\s)', text)\n",
        "#     preProcessed = [\n",
        "#         item.strip() for item in preProcessed if item.strip() # remove white spaces\n",
        "#     ]\n",
        "#     encoded = [self.strToInt[s] for s in preProcessed]\n",
        "#     return encoded\n",
        "\n",
        "#   def decode(self, encoded):\n",
        "#     text = \" \".join([self.intToStr[i] for i in encoded]) # Join Indivisual Tokens\n",
        "#     text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) # Replace Spaces before the specified Puncuations\n",
        "#     return text"
      ],
      "metadata": {
        "id": "kX20jR1sQnS3"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenization = Tokenizer(vocabDict) # Object Creation\n",
        "sampleText = \"\"\"\"It's the last he painted, you know,\"\n",
        "                 Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "encodedTokenId = tokenization.encode(sampleText)\n",
        "print(encodedTokenId)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m63Lf_HATlZY",
        "outputId": "3bb8697f-fbcf-49d2-8673-2826cb84d803"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 113, 54, 1103, 699, 614, 853, 65, 1251, 691, 65, 1, 124, 67, 93, 962, 1231, 863, 905, 67]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenization.decode(encodedTokenId)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "x33ZwU6FWzFV",
        "outputId": "f0856170-9c5a-4ec0-d894-ba6288c6f91a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\" It\\' the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What will Happen If Word is not present in Vocab Dictionary and encode method is called?\n",
        "In the updated Tokenizer, the encode method uses [self.strToInt[s] for s in preProcessed if s in self.strToInt]. This filters out any tokens not in the vocabulary, so only known words are encoded. Unknown words are ignored, which prevents KeyError but means they aren’t represented in the output. Essentially, the encoder “works” without errors, but it does not actually encode words missing from the vocabDict"
      ],
      "metadata": {
        "id": "sUY6qAudYWFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testText = \"Hello xyz, do you like tea?\"\n",
        "print(tokenization.encode(testText))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpraacuBXKKC",
        "outputId": "b9844c08-30fa-455a-f93c-15c576e13e68"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[65, 434, 1251, 727, 1089, 71]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SPECIAL CONTEXT TOKENS\n",
        "Helps in dealing with words/tokens not present in vocabdict\n",
        "\n",
        "<|unk|> and <|endoftext|>\n"
      ],
      "metadata": {
        "id": "Y2etoU4LZHyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newTokens = sorted(list(set(preProcessed)))\n",
        "newTokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "newVocabSize = len(newTokens)\n",
        "print(newVocabSize) # Previous size was 1258\n",
        "newVocabDict = {token:integer for integer, token in enumerate(newTokens)}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JumZ794QYboE",
        "outputId": "7b257fd1-53f4-4782-afc6-926bf9c5ea92"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1260\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(list(newVocabDict.items())[-5:]):\n",
        "  print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYkC0juwbUUr",
        "outputId": "47530a52-b849-4cb3-c9ce-42d8a0cbdd6a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('younger', 1255)\n",
            "('your', 1256)\n",
            "('yourself', 1257)\n",
            "('<|endoftext|>', 1258)\n",
            "('<|unk|>', 1259)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenizerV2:\n",
        "  def __init__(self, vocabDict):  # Constructor\n",
        "        self.strToInt = vocabDict  # Encoder dictionary\n",
        "        self.intToStr = {i: s for s, i in vocabDict.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    preProcessed = re.split(r'[,.:;/_!\"()\\'] |--|\\s', text)\n",
        "    preProcessed = [item.strip() for item in preProcessed if item.strip()]\n",
        "    preProcessed = [\n",
        "        item if item in self.strToInt\n",
        "        else \"<|unk|>\" for item in preProcessed  # Assign this token\n",
        "    ]\n",
        "    encoded = [self.strToInt[s] for s in preProcessed]\n",
        "    return encoded\n",
        "\n",
        "  def decode(self, encoded):\n",
        "    text = \" \".join([self.intToStr[i] for i in encoded])\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text\n",
        ""
      ],
      "metadata": {
        "id": "HKOrauWobuyy"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizationV2 = TokenizerV2(newVocabDict)\n",
        "text1 = \"Hello, do you like tea?\"  # Text source 1\n",
        "text2 = \"In the sunlit terraces of the palace\" # Text source 2\n",
        "\n",
        "txt = \" <|endoftext|> \".join((text1, text2))\n",
        "print(txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrjzywvMdDER",
        "outputId": "8ecb9408-9f96-43f9-f7cb-b704bb7ab2bf"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizationV2.encode(txt)\n",
        "# 1258 wherever there is an unknown token/word\n",
        "# 1259 for end of text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxbYGYHAdluF",
        "outputId": "9cf64fed-2ebd-4862-d5c1-8c2b4c91df86"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1259, 434, 1251, 727, 1259, 1258, 112, 1103, 1071, 1098, 825, 1103, 1259]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizationV2.decode(tokenizationV2.encode(txt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "gAStcLVMd-UH",
        "outputId": "f10884b5-4dbf-4cfc-f76b-bcb84775e5de"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|unk|> do you like <|unk|> <|endoftext|> In the sunlit terraces of the <|unk|>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    }
  ]
}