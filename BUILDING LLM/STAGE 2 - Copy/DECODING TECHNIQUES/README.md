# ğŸš€ Language Model Training, Evaluation & Decoding Strategies  

This project implements a **GPT-like Transformer model** from scratch in PyTorch and demonstrates **training, evaluation, and decoding strategies**.  

---

##  1. Model Components  

### ğŸ”¹ Layer Normalization  
- Normalizes each tokenâ€™s embedding to stabilize training.  
- Uses two trainable parameters:  
  - **Scale (Î³)** â†’ adjusts magnitude  
  - **Shift (Î²)** â†’ re-centers distribution  
- Helps reduce **internal covariate shift** and speeds up convergence.  

---

### ğŸ”¹ GELU Activation  
- **Gaussian Error Linear Unit (GELU)** is used instead of ReLU.  
- Provides **smooth non-linearity**, letting the model capture subtle relationships.  
- Works better than ReLU/Tanh in large-scale Transformers.  

---

### ğŸ”¹ Feed-Forward Network (FFN)  
- Each Transformer block has an **expansion â†’ non-linearity â†’ compression** flow.  
- Expansion: Embedding dimension â†’ 4x larger.  
- Activation: GELU applied.  
- Compression: Projected back to original embedding size.  
- Adds **capacity** for richer feature representation.  

---

### ğŸ”¹ Multi-Head Self Attention (MHSA)  
- Core mechanism of Transformers.  
- Splits embeddings into multiple **heads** to attend to different parts of context.  
- **Causal Masking** ensures tokens cannot â€œsee the futureâ€.  
- Produces **context vectors** that capture relationships across tokens.  

---

### ğŸ”¹ Transformer Block  
- Combines **Multi-Head Attention + FFN** with:  
  - **Layer Normalization**  
  - **Residual (Skip) Connections**  
  - **Dropout for regularization**  
- Stacking multiple blocks deepens model capacity.  

---

### ğŸ”¹ GPT Model Architecture  
- Components:  
  1. **Token Embeddings** â†’ convert IDs to vectors.  
  2. **Positional Embeddings** â†’ inject sequence order info.  
  3. **Stacked Transformer Blocks** â†’ core computation.  
  4. **Final Linear Head** â†’ maps embeddings back to vocabulary logits.  

---

##  2. Dataset Preparation  

- Raw text is tokenized into **GPT-2 tokens**.  
- A **sliding window** approach creates overlapping input-target pairs.  
- **Training set** â†’ 90% of data  
- **Validation set** â†’ 10% of data  
- Each batch provides input tokens (X) and target tokens (Y).  

---

##  3. Training & Evaluation  

- **Loss Function** â†’ Cross Entropy (compares predicted logits with true tokens).  
- **Optimizer** â†’ AdamW (weight decay for regularization).  
- **Overfitting Check**:  
  - Training loss << Validation loss â†’ Overfitting.  
- **Metrics**:  
  - Training Loss  
  - Validation Loss  
  - Tokens Seen (progress measure)  

- Visualization: Loss curves vs **epochs** and **tokens seen**.  

---

## 4. Decoding Strategies  

Once trained, the model generates text using **decoding strategies**.  

### ğŸ”¹ A) Greedy Decoding  
- Always selects the token with **highest probability**.  
- Deterministic â†’ same output every time.  
- Limitation â†’ **Repetitive & dull outputs**.  

---

### ğŸ”¹ B) Temperature Scaling (ğŸŒ¡ï¸ Hyperparameter)  
- Controls **randomness** in sampling.  
- Formula â†’ `Softmax(logits / T)`  
- Effects of Temperature (T):  
  - **T â†’ 0** â†’ Sharp distribution â†’ behaves like greedy decoding.  
  - **T = 1** â†’ Normal probabilities (default).  
  - **T > 1** â†’ Flatter distribution â†’ more diverse but less accurate outputs.  
-  Balances **determinism vs creativity**.  

---

### ğŸ”¹ C) Top-K Sampling  
- Restricts sampling to **top K most probable tokens**.  
- Remaining logits replaced with `-âˆ` (ignored).  
- Ensures:  
  - No sampling from very low-probability words.  
  - Improves coherence while keeping diversity.  

---

### ğŸ”¹ D) Combined (Top-K + Temperature)  
- Typical modern strategy:  
  1. **Filter tokens using Top-K**.  
  2. **Rescale with Temperature**.  
  3. **Sample from multinomial distribution**.  
-  Ensures **coherence** (via Top-K) and **creativity** (via Temperature).  

---

##  5. Key Insights  

1. **Training Loss vs Validation Loss** â†’ indicates generalization ability.  
2. **Decoding Strategy** drastically affects **quality of text**.  
3. **Temperature (T)** is a **hyperparameter** that controls creativity.  
4. **Top-K filtering** avoids random, nonsensical tokens.  
5. Final pipeline:  Logits â†’ Top-K filtering â†’ Temperature scaling â†’ Softmax â†’ Multinomial sampling

