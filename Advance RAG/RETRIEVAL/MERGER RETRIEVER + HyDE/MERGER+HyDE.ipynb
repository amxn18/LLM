{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhXlPLZl0mAw"
      },
      "outputs": [],
      "source": [
        "%pip install flashrank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqZM5P2J3sv0"
      },
      "outputs": [],
      "source": [
        "%pip install langchain langchain-community langchain-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNyYRq_O4RmR"
      },
      "outputs": [],
      "source": [
        "%pip install pypdf tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvattsXe5hBO"
      },
      "outputs": [],
      "source": [
        "%pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30ZCRxAl6jSY"
      },
      "outputs": [],
      "source": [
        "%pip install rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4p_dMaz5h-Bd"
      },
      "outputs": [],
      "source": [
        "%pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K-ambBG-yB4"
      },
      "source": [
        "## STEP 1 (DATA INGESTION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "BmwFTQYL1fIm",
        "outputId": "3f2dc5f7-2e40-4e4d-ef49-1b7b49fad671"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bb4af6ac-c3df-4a17-8bc4-4c1343b88bbf\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bb4af6ac-c3df-4a17-8bc4-4c1343b88bbf\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving nlp.pdf to nlp.pdf\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nYj0oAJM3fOP"
      },
      "outputs": [],
      "source": [
        "filePath = \"/content/nlp.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MAZEiz-335jf"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0l-qRvNH3oUF"
      },
      "outputs": [],
      "source": [
        "def loadDocs(path:str):\n",
        "  loader = PyPDFLoader(path)\n",
        "  docs = loader.load()\n",
        "  return docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dCaO98l4LFV",
        "outputId": "41910882-4825-449a-ce2b-13134ca8aa3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "19\n",
            "page_content='Retrieval-Augmented Generation for\n",
            "Knowledge-Intensive NLP Tasks\n",
            "Patrick Lewis†‡, Ethan Perez⋆,\n",
            "Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\n",
            "Mike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†\n",
            "†Facebook AI Research;‡University College London;⋆New York University;\n",
            "plewis@fb.com\n",
            "Abstract\n",
            "Large pre-trained language models have been shown to store factual knowledge\n",
            "in their parameters, and achieve state-of-the-art results when ﬁne-tuned on down-\n",
            "stream NLP tasks. However, their ability to access and precisely manipulate knowl-\n",
            "edge is still limited, and hence on knowledge-intensive tasks, their performance\n",
            "lags behind task-speciﬁc architectures. Additionally, providing provenance for their\n",
            "decisions and updating their world knowledge remain open research problems. Pre-\n",
            "trained models with a differentiable access mechanism to explicit non-parametric\n",
            "memory have so far been only investigated for extractive downstream tasks. We\n",
            "explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation\n",
            "(RAG) — models which combine pre-trained parametric and non-parametric mem-\n",
            "ory for language generation. We introduce RAG models where the parametric\n",
            "memory is a pre-trained seq2seq model and the non-parametric memory is a dense\n",
            "vector index of Wikipedia, accessed with a pre-trained neural retriever. We com-\n",
            "pare two RAG formulations, one which conditions on the same retrieved passages\n",
            "across the whole generated sequence, and another which can use different passages\n",
            "per token. We ﬁne-tune and evaluate our models on a wide range of knowledge-\n",
            "intensive NLP tasks and set the state of the art on three open domain QA tasks,\n",
            "outperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract\n",
            "architectures. For language generation tasks, we ﬁnd that RAG models generate\n",
            "more speciﬁc, diverse and factual language than a state-of-the-art parametric-only\n",
            "seq2seq baseline.\n",
            "1 Introduction\n",
            "Pre-trained neural language models have been shown to learn a substantial amount of in-depth knowl-\n",
            "edge from data [47]. They can do so without any access to an external memory, as a parameterized\n",
            "implicit knowledge base [51, 52]. While this development is exciting, such models do have down-\n",
            "sides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into\n",
            "their predictions, and may produce “hallucinations” [ 38]. Hybrid models that combine parametric\n",
            "memory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these\n",
            "issues because knowledge can be directly revised and expanded, and accessed knowledge can be\n",
            "inspected and interpreted. REALM [ 20] and ORQA [ 31], two recently introduced models that\n",
            "combine masked language models [8] with a differentiable retriever, have shown promising results,\n",
            "arXiv:2005.11401v4  [cs.CL]  12 Apr 2021' metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/nlp.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1'}\n"
          ]
        }
      ],
      "source": [
        "docs = loadDocs(filePath)\n",
        "print(len(docs))\n",
        "print(docs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NC-DK0xR4cyY"
      },
      "outputs": [],
      "source": [
        "def splitDocs(docs):\n",
        "  splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size = 400,\n",
        "      chunk_overlap = 150\n",
        "  )\n",
        "  chunks = splitter.split_documents(docs)\n",
        "  return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZk4u8Lg4uqg",
        "outputId": "d7b1ab21-244e-44bd-b13c-115749b6fda5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "262\n",
            "page_content='Retrieval-Augmented Generation for\n",
            "Knowledge-Intensive NLP Tasks\n",
            "Patrick Lewis†‡, Ethan Perez⋆,\n",
            "Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\n",
            "Mike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†\n",
            "†Facebook AI Research;‡University College London;⋆New York University;\n",
            "plewis@fb.com\n",
            "Abstract' metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/nlp.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1'}\n"
          ]
        }
      ],
      "source": [
        "chunks = splitDocs(docs)\n",
        "print(len(chunks))\n",
        "print(chunks[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qMdyDfyU4gKO"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings, ChatHuggingFace, HuggingFaceEndpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_UYWfqZ5Pas"
      },
      "outputs": [],
      "source": [
        "# HF API "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChqbGWVt5BC6"
      },
      "outputs": [],
      "source": [
        "embedding = HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "UU7S5k-b60Mb"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace, HuggingFacePipeline\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id = \"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    task = \"text-generation\",\n",
        ")\n",
        "model = ChatHuggingFace(llm = llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ijUsbmxD95ia"
      },
      "outputs": [],
      "source": [
        "parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5RNDoLmRgGC"
      },
      "source": [
        "### STEP 2 (SELF QUERYING ANALYZER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "qii99YBVRm85"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "import json\n",
        "selfQueryPrompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "You are a query analyzer.\n",
        "\n",
        "Your task is to split the question into:\n",
        "1. semantic_query → what should be searched semantically\n",
        "2. filters → structured metadata constraints\n",
        "\n",
        "Allowed filters:\n",
        "- page (integer)\n",
        "\n",
        "RULES:\n",
        "- If the question mentions \"page X\", extract page = X\n",
        "- Remove filter-related words from the semantic query\n",
        "- If no filters apply, return an empty filters object\n",
        "\n",
        "EXAMPLES:\n",
        "\n",
        "Question:\n",
        "\"What is discussed on page 6 about Jeopardy Question Generation?\"\n",
        "\n",
        "Output:\n",
        "{{\n",
        "  \"semantic_query\": \"Jeopardy Question Generation\",\n",
        "  \"filters\": {{ \"page\": 6 }}\n",
        "}}\n",
        "\n",
        "Question:\n",
        "\"Explain Jeopardy Question Generation\"\n",
        "\n",
        "Output:\n",
        "{{\n",
        "  \"semantic_query\": \"Jeopardy Question Generation\",\n",
        "  \"filters\": {{}}\n",
        "}}\n",
        "\n",
        "NOW ANALYZE THIS QUESTION:\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Return ONLY valid JSON.\n",
        "\"\"\",\n",
        "    input_variables=[\"question\"]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "rkSVlzJbTD39"
      },
      "outputs": [],
      "source": [
        "def selfQueryAnalyzer(question):\n",
        "    chain = selfQueryPrompt | model | parser\n",
        "    response = chain.invoke({\"question\": question})\n",
        "\n",
        "    try:\n",
        "        parsed = json.loads(response)\n",
        "        semanticQuery = parsed.get(\"semantic_query\", question)\n",
        "        filters = parsed.get(\"filters\", {})\n",
        "    except Exception as e:\n",
        "        print(\"Parsing failed:\", e)\n",
        "        semanticQuery = question\n",
        "        filters = {}\n",
        "\n",
        "    return semanticQuery, filters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0P35Zd2ITyjP",
        "outputId": "0c1f78bb-7dd7-440a-f5e3-9472e52bf329"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Semantic Query: Jeopardy Question Generation\n",
            "Filters: {'page': 6}\n"
          ]
        }
      ],
      "source": [
        "q1 = \"What is discussed on page 6 about Jeopardy Question Generation?\"\n",
        "q2 = \"Explain Jeopardy Question Generation\"\n",
        "q3 = \"What are the results in Table 2?\"\n",
        "q4 = \"What are the models discussed on page no 3,4 of this research paper?\"\n",
        "semantic_query, filters = selfQueryAnalyzer(q1)\n",
        "\n",
        "print(\"Semantic Query:\", semantic_query)\n",
        "print(\"Filters:\", filters)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-BT80vSrm4x"
      },
      "source": [
        "### STEP 3 (HYPOTHETICAL DOCUMENT EMBEDDING)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "l7iwYBSQrmZy"
      },
      "outputs": [],
      "source": [
        "HyDEPrompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "You are generating a hypothetical passage that could appear inside a research paper.\n",
        "\n",
        "STRICT RULES:\n",
        "- Write in academic / research-paper style\n",
        "- Use terminology that would realistically appear in the given document\n",
        "- Do NOT introduce methods, models, or techniques not mentioned in the paper\n",
        "- Do NOT explain generally like a textbook\n",
        "- Do NOT mention GANs, templates, or unrelated NLP methods\n",
        "- Focus on factual, descriptive language\n",
        "\n",
        "TASK:\n",
        "Write a short paragraph (5–7 sentences) that could plausibly appear in the paper\n",
        "to address the following topic.\n",
        "\n",
        "Topic:\n",
        "{semantic_query}\n",
        "\n",
        "Hypothetical passage:\n",
        "\"\"\",\n",
        "    input_variables=[\"semantic_query\"]\n",
        ")\n",
        "\n",
        "\n",
        "def GenerateHyDE(semantic_query):\n",
        "    chain = HyDEPrompt | model | parser\n",
        "    hypotheticalDoc = chain.invoke(\n",
        "        {\"semantic_query\": semantic_query}\n",
        "    )\n",
        "    return hypotheticalDoc.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gk4CJBj5sxFZ",
        "outputId": "d770c2d6-74f1-45de-ce49-697ba3425d4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Semantic Query:\n",
            "\n",
            "Jeopardy Question Generation\n",
            "\n",
            "HyDE Output:\n",
            "The Jeopardy question generation task has garnered significant attention in recent years, with various studies investigating the optimization of question form and difficulty. Research has shown that the performance of Jeopardy contestants is strongly correlated with the quality and relevance of the generated questions (Keller et al., 2019). In an effort to improve question quality, this study employs a revised version of the \"Question Form Generator\" (QFG) algorithm, which incorporates a weighted combination of linguistic features, including part-of-speech tags and sentence structures, to produce more informative and nuanced questions. By analyzing the correlation between question difficulty and contestant performance, we have identified a significant relationship between the QFG's ability to generate questions with higher median difficulty and top-performing contestants. Our findings suggest that the QFG algorithm has the potential to improve Jeopardy contestant performance, and we propose future research directions to further investigate the role of question form in Jeopardy success.\n",
            "1102\n"
          ]
        }
      ],
      "source": [
        "question = \"What is discussed on page 6 about Jeopardy Question Generation?\"\n",
        "\n",
        "semantic_query, filters = selfQueryAnalyzer(question)\n",
        "\n",
        "hydeDoc = GenerateHyDE(semantic_query)\n",
        "\n",
        "print(\"Semantic Query:\")\n",
        "print()\n",
        "print(semantic_query)\n",
        "print()\n",
        "print(\"HyDE Output:\")\n",
        "print(hydeDoc)\n",
        "print(len(hydeDoc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiPN3YMp--yq"
      },
      "source": [
        "## STEP 4 (MERGER RETRIEVER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "oqhFP3DYymhs"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "vectorStore = Chroma.from_documents(chunks, embedding)\n",
        "denseRetriever = vectorStore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "MLeX7Jqyyppd"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "sparseRetriever = BM25Retriever.from_documents(chunks)\n",
        "sparseRetriever.k = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "sJz8bq9o2WgJ"
      },
      "outputs": [],
      "source": [
        "def denseRetrieverWithHyDE(semantic_query, hyde_doc, denseRetriever):\n",
        "    combined_query = semantic_query + \"\\n\" + hyde_doc\n",
        "    return denseRetriever.invoke(combined_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "4KJ981fx2rJP"
      },
      "outputs": [],
      "source": [
        "def sparseRetrieverOnly(semantic_query, sparseRetriever):\n",
        "    return sparseRetriever.invoke(semantic_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "LhmCerPH3FKw"
      },
      "outputs": [],
      "source": [
        "def metadataRetriever(all_chunks, filters):\n",
        "    if not filters:\n",
        "        return []\n",
        "\n",
        "    results = []\n",
        "    for doc in all_chunks:\n",
        "        keep = True\n",
        "        for key, value in filters.items():\n",
        "            if doc.metadata.get(key) != value:\n",
        "                keep = False\n",
        "                break\n",
        "        if keep:\n",
        "            results.append(doc)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "ocka7mc16ucT"
      },
      "outputs": [],
      "source": [
        "def MergerRetriever(\n",
        "    semantic_query,\n",
        "    hyde_doc,\n",
        "    denseRetriever,\n",
        "    sparseRetriever,\n",
        "    all_chunks,\n",
        "    filters,\n",
        "    weights=(0.4, 0.4, 0.2),\n",
        "    rrf_k=60\n",
        "):\n",
        "    scores = {}\n",
        "    doc_map = {}\n",
        "\n",
        "    # Dense\n",
        "    dense_docs = denseRetrieverWithHyDE(\n",
        "        semantic_query, hyde_doc, denseRetriever\n",
        "    )\n",
        "    for rank, doc in enumerate(dense_docs):\n",
        "        key = doc.page_content\n",
        "        doc_map[key] = doc\n",
        "        scores[key] = scores.get(key, 0) + weights[0] / (rank + 1 + rrf_k)\n",
        "\n",
        "    # Sparse\n",
        "    sparse_docs = sparseRetrieverOnly(\n",
        "        semantic_query, sparseRetriever\n",
        "    )\n",
        "    for rank, doc in enumerate(sparse_docs):\n",
        "        key = doc.page_content\n",
        "        doc_map[key] = doc\n",
        "        scores[key] = scores.get(key, 0) + weights[1] / (rank + 1 + rrf_k)\n",
        "\n",
        "    # Metadata\n",
        "    meta_docs = metadataRetriever(all_chunks, filters)\n",
        "    for rank, doc in enumerate(meta_docs):\n",
        "        key = doc.page_content\n",
        "        doc_map[key] = doc\n",
        "        scores[key] = scores.get(key, 0) + weights[2] / (rank + 1 + rrf_k)\n",
        "\n",
        "    ranked = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    return [doc_map[k] for k, _ in ranked]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jbVeXAXZSbM",
        "outputId": "92e1b4f8-b6b6-41e2-c289-96c7d238487d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "28\n",
            "recent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being\n",
            "extractive tasks, we ﬁnd that unconstrained generation outperforms previous extractive approaches.\n",
            "For knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question\n",
            "generation\n"
          ]
        }
      ],
      "source": [
        "question = \"What is discussed on page 6 about Jeopardy Question Generation?\"\n",
        "\n",
        "semantic_query, filters = selfQueryAnalyzer(question)\n",
        "hyde_doc = GenerateHyDE(semantic_query)\n",
        "\n",
        "mergedDocs = MergerRetriever(\n",
        "    semantic_query=semantic_query,\n",
        "    hyde_doc=hyde_doc,\n",
        "    denseRetriever=denseRetriever,\n",
        "    sparseRetriever=sparseRetriever,\n",
        "    all_chunks=chunks,\n",
        "    filters=filters\n",
        ")\n",
        "\n",
        "print(len(mergedDocs))\n",
        "print(mergedDocs[0].page_content[:300])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC4IeChO_E-I"
      },
      "source": [
        "## STEP 5 (FLASH RERANKING)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqgHbk4yRJSj"
      },
      "outputs": [],
      "source": [
        "!pip install flashrank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "TbAHbohsRQWE"
      },
      "outputs": [],
      "source": [
        "from flashrank.Ranker import Ranker, RerankRequest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "rjp06GhVbXY4"
      },
      "outputs": [],
      "source": [
        "from flashrank.Ranker import Ranker, RerankRequest\n",
        "def reranking(query, passages, choice):\n",
        "    if choice == \"Nano\":\n",
        "        ranker = Ranker()\n",
        "    elif choice == \"Small\":\n",
        "        ranker = Ranker(model_name=\"ms-marco-MiniLM-L-12-v2\", cache_dir=\"/opt\")\n",
        "    elif choice == \"Medium\":\n",
        "        ranker = Ranker(model_name=\"rank-T5-flan\", cache_dir=\"/opt\")\n",
        "    elif choice == \"Large\":\n",
        "        ranker = Ranker(model_name=\"ms-marco-MultiBERT-L-12\", cache_dir=\"/opt\")\n",
        "\n",
        "    rerankRequest = RerankRequest(\n",
        "        query=query,\n",
        "        passages=passages\n",
        "    )\n",
        "\n",
        "    results = ranker.rerank(rerankRequest)\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5aDZdg0bhnp",
        "outputId": "ae4278c1-f58c-4ea1-dc41-ff81d8b338c0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': 0,\n",
              " 'text': 'recent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being\\nextractive tasks, we ﬁnd that unconstrained generation outperforms previous extractive approaches.\\nFor knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question\\ngeneration, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and'}"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "passages = [\n",
        "    {\"id\": i, \"text\": doc.page_content}\n",
        "    for i, doc in enumerate(mergedDocs)\n",
        "]\n",
        "passages[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChIouBGJbodo",
        "outputId": "198755bb-d357-41e4-fb6f-683029dc3196"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "rank-T5-flan.zip: 100%|██████████| 73.7M/73.7M [00:01<00:00, 54.0MiB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "28\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'id': 4,\n",
              " 'text': 'Fact Veriﬁcation: Fact Query\\nsupports\\t(y)\\nQuestion Generation\\nFact Veriﬁcation:\\nLabel Generation\\nDocument\\nIndex\\nDefine\\t\"middle\\tear\"(x)\\nQuestion Answering:\\nQuestion Query\\nThe\\tmiddle\\tear\\tincludes\\nthe\\ttympanic\\tcavity\\tand\\nthe\\tthree\\tossicles.\\t\\t(y)\\nQuestion Answering:\\nAnswer GenerationRetriever pη\\n(Non-Parametric)\\nz4\\nz3\\nz2\\nz1\\nd(z)\\nJeopardy Question\\nGeneration:\\nAnswer Query',\n",
              " 'score': np.float32(0.65737104)}"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rerankingResults = reranking(\n",
        "    query=semantic_query,\n",
        "    passages=passages,\n",
        "    choice=\"Medium\"\n",
        ")\n",
        "print(len(rerankingResults))\n",
        "rerankingResults[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PwJa5yPSYDf"
      },
      "source": [
        "### STEP 6 (CROSS-ENCODERS) RERANKING AGAIN FOR MORE ACCURATE RESULTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CHF8IJl6uxm",
        "outputId": "ebe5d2d1-1d11-4a44-e1de-c20650cd3d36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page_content='Fact Veriﬁcation: Fact Query\n",
            "supports\t(y)\n",
            "Question Generation\n",
            "Fact Veriﬁcation:\n",
            "Label Generation\n",
            "Document\n",
            "Index\n",
            "Define\t\"middle\tear\"(x)\n",
            "Question Answering:\n",
            "Question Query\n",
            "The\tmiddle\tear\tincludes\n",
            "the\ttympanic\tcavity\tand\n",
            "the\tthree\tossicles.\t\t(y)\n",
            "Question Answering:\n",
            "Answer GenerationRetriever pη\n",
            "(Non-Parametric)\n",
            "z4\n",
            "z3\n",
            "z2\n",
            "z1\n",
            "d(z)\n",
            "Jeopardy Question\n",
            "Generation:\n",
            "Answer Query' metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/nlp.pdf', 'total_pages': 19, 'page': 1, 'page_label': '2'}\n"
          ]
        }
      ],
      "source": [
        "flashDocs = [mergedDocs[item[\"id\"]] for item in rerankingResults]\n",
        "print(flashDocs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AjY8VMuc3QK"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "crossEncoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "pairs = [[question, doc.page_content] for doc in flashDocs]\n",
        "\n",
        "scores = crossEncoder.predict(pairs)\n",
        "crossReranked = list(zip(scores, flashDocs))\n",
        "crossReranked = sorted(crossReranked, reverse=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODgzxLJwSpFe",
        "outputId": "f58fa565-ce75-4aac-e8bb-6be72a891d8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "28\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['What is discussed on page 6 about Jeopardy Question Generation?',\n",
              " 'Fact Veriﬁcation: Fact Query\\nsupports\\t(y)\\nQuestion Generation\\nFact Veriﬁcation:\\nLabel Generation\\nDocument\\nIndex\\nDefine\\t\"middle\\tear\"(x)\\nQuestion Answering:\\nQuestion Query\\nThe\\tmiddle\\tear\\tincludes\\nthe\\ttympanic\\tcavity\\tand\\nthe\\tthree\\tossicles.\\t\\t(y)\\nQuestion Answering:\\nAnswer GenerationRetriever pη\\n(Non-Parametric)\\nz4\\nz3\\nz2\\nz1\\nd(z)\\nJeopardy Question\\nGeneration:\\nAnswer Query']"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(len(pairs))\n",
        "print()\n",
        "pairs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtBO8Bvl7eiM",
        "outputId": "46d59ad4-bc83-41d6-f2e9-0ceeddedfa3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "28\n",
            "3.2308133\n",
            "BART for Jeopardy question generation. Following recent work on diversity-promoting decoding\n",
            "[33, 59, 39], we also investigate generation diversity by calculating the ratio of distinct ngrams to\n",
            "total\n"
          ]
        }
      ],
      "source": [
        "print(len(crossReranked))\n",
        "print(crossReranked[0][0])   # Relevence score\n",
        "print(crossReranked[0][1].page_content[:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MJiDUpM7mEw",
        "outputId": "9e716297-a6f6-4b12-c3d4-b82b31c1ab2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5\n",
            "BART for Jeopardy question generation. Following recent work on diversity-promoting decoding\n",
            "[33, 59, 39], we also investigate generation diversity by calculating the ratio of distinct ngrams to\n",
            "total\n"
          ]
        }
      ],
      "source": [
        "K = 5\n",
        "finalRerankedDocs = [doc for _, doc in crossReranked[:K]]\n",
        "print(len(finalRerankedDocs))\n",
        "print(finalRerankedDocs[0].page_content[:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6MlzQiNmzx6"
      },
      "source": [
        "### STEP 7 (WINDOW SEARCH RETRIEVER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "SZ2JTtOwSqKY"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "def BuildSentenceIndex(allChunks):\n",
        "    pageSentences = defaultdict(list)\n",
        "\n",
        "    for chunk in allChunks:\n",
        "        page = chunk.metadata.get(\"page\")\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', chunk.page_content)\n",
        "\n",
        "        for s in sentences:\n",
        "            clean = s.strip()\n",
        "            if clean:\n",
        "                pageSentences[page].append(clean)\n",
        "\n",
        "    return pageSentences\n",
        "\n",
        "\n",
        "sentenceIndex = BuildSentenceIndex(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "OUEd9y4VStdS"
      },
      "outputs": [],
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "def SentenceWindowRetriever(rankedChunks,sentenceIndex,windowSize=2):\n",
        "    expandedDocs = []\n",
        "\n",
        "    for doc in rankedChunks:\n",
        "        page = doc.metadata.get(\"page\")\n",
        "        if page not in sentenceIndex:\n",
        "            expandedDocs.append(doc)\n",
        "            continue\n",
        "\n",
        "        fullSentences = sentenceIndex[page] # Sentences from entire page not chunk\n",
        "\n",
        "        chunkSentences = re.split(r'(?<=[.!?])\\s+',doc.page_content) # Splitting retreived chunk into sentences\n",
        "\n",
        "        indices = []\n",
        "\n",
        "        # locating sentence positions in full document\n",
        "        for i, sent in enumerate(fullSentences):\n",
        "            for cs in chunkSentences:\n",
        "                if cs.strip() and cs.strip() in sent:\n",
        "                    indices.append(i)\n",
        "\n",
        "        if not indices:\n",
        "            expandedDocs.append(doc)\n",
        "            continue\n",
        "\n",
        "        start = max(0, min(indices) - windowSize)\n",
        "        end = min(len(fullSentences), max(indices) + windowSize + 1)\n",
        "\n",
        "        windowed_sentences = fullSentences[start:end]\n",
        "\n",
        "        expandedDocs.append(\n",
        "            Document(\n",
        "                page_content=\" \".join(dict.fromkeys(windowed_sentences)),\n",
        "                metadata=doc.metadata\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return expandedDocs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIMZ6E1KQN6n",
        "outputId": "7ef0e8ff-e563-47df-8e5e-d7c2d995e3b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original length: 384\n",
            "Windowed length: 1075\n",
            "\n",
            "ORIGINAL:\n",
            " BART for Jeopardy question generation. Following recent work on diversity-promoting decoding\n",
            "[33, 59, 39], we also investigate generation diversity by calculating the ratio of distinct ngrams to\n",
            "total ngrams generated by different models. Table 5 shows that RAG-Sequence’s generations are\n",
            "more diverse than RAG-Token’s, and both are signiﬁcantly more diverse than BART without needing\n",
            "\n",
            "AFTER WINDOWING\n",
            "\n",
            "We ﬁnd that the top retrieved document is from a gold article\n",
            "in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases. in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases. 4.5 Additional Results\n",
            "Generation Diversity Section 4.3 shows that RAG models are more factual and speciﬁc than\n",
            "BART for Jeopardy question generation. Following recent work on diversity-promoting decoding BART for Jeopardy question generation. Following recent work on diversity-promoting decoding\n",
            "[33, 59, 39], we also investigate generation diversity by calculating the ratio of distinct ngrams to\n",
            "total ngrams generated by different models. Table 5 shows that RAG-Sequence’s generations are\n",
            "more diverse than RAG-Token’s, and both are signiﬁcantly more diverse than BART without needing more diverse than RAG-Token’s, and both are signiﬁcantly more diverse than BART without needing\n",
            "any diversity-promoting decoding. Retrieval Ablations A key feature of RAG is learning to retrieve relevant information for the task.\n"
          ]
        }
      ],
      "source": [
        "print(\"Original length:\", len(finalRerankedDocs[0].page_content))\n",
        "\n",
        "windowDocs = SentenceWindowRetriever(\n",
        "    rankedChunks=finalRerankedDocs,\n",
        "    sentenceIndex=sentenceIndex,\n",
        "    windowSize=2\n",
        ")\n",
        "\n",
        "print(\"Windowed length:\", len(windowDocs[0].page_content))\n",
        "\n",
        "print(\"\\nORIGINAL:\\n\", finalRerankedDocs[0].page_content)\n",
        "print()\n",
        "print(\"AFTER WINDOWING\")\n",
        "print()\n",
        "print(windowDocs[0].page_content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0NiOddTYvaT",
        "outputId": "9a1a63e1-f274-4bef-944a-196e1bb59ef5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(windowDocs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KO_9bberXrAV"
      },
      "source": [
        "### STEP 8 (PARENT DOCUMENT RETREIVER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "XzLJVvw3nEV4"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "def ParentDocumentRetriever(ranked_docs,all_chunks):  # Topk docs and all chunks\n",
        "\n",
        "    page_to_chunks = defaultdict(list)\n",
        "\n",
        "    for chunk in all_chunks:\n",
        "        page = chunk.metadata.get(\"page\")\n",
        "        page_to_chunks[page].append(chunk)\n",
        "\n",
        "    parent_docs = []\n",
        "    seen_pages = set()\n",
        "\n",
        "    for doc in ranked_docs:\n",
        "        page = doc.metadata.get(\"page\")\n",
        "\n",
        "        if page in seen_pages:\n",
        "            continue\n",
        "\n",
        "        seen_pages.add(page)\n",
        "\n",
        "        page_chunks = page_to_chunks.get(page, [])\n",
        "\n",
        "        merged_text = \" \".join(\n",
        "            chunk.page_content for chunk in page_chunks\n",
        "        )\n",
        "\n",
        "        parent_docs.append(\n",
        "            Document(\n",
        "                page_content=merged_text,\n",
        "                metadata=doc.metadata\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return parent_docs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uf9l9XaxpiPj",
        "outputId": "9f8da15a-472c-4520-b2bb-576c3c13f41e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5\n",
            "Windowed length: 1075\n",
            "Parent length: 5803\n",
            "\n",
            "PARENT DOC SAMPLE:\n",
            "\n",
            "Document 1: his works are considered classics of American\n",
            "literature ... His wartime experiences formed the basis for his novel\n",
            "”A Farewell to Arms” (1929) ...\n",
            "Document 2: ... artists of the 1920s ”Lost Generation” expatriate\n",
            "community . His debut novel, ”The Sun Also Rises” , was published\n",
            "in 1926.\n",
            "BOS\n",
            "”\n",
            "TheSunAlso\n",
            "R ises\n",
            "” is a\n",
            "novel\n",
            "by this\n",
            "author\n",
            "of ” A\n",
            "Farewellto\n",
            "Arms\n",
            "”\n",
            "Doc 1\n",
            "Doc 2\n",
            "Doc 3 in 1926.\n",
            "BOS\n",
            "”\n",
            "TheSunAlso\n",
            "R ises\n",
            "” is a\n",
            "novel\n",
            "by this\n",
            "author\n",
            "of ” A\n",
            "Farewellto\n",
            "Arms\n",
            "”\n",
            "Doc 1\n",
            "Doc 2\n",
            "Doc 3\n",
            "Doc 4\n",
            "Doc 5\n",
            "Figure 2: RAG-Token document posteriorp(zi|x,yi,y−i) for each generated token for input “Hem-\n",
            "ingway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high ingway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high\n",
            "when generating “A Farewell to Arms\" and for document 2 when generating “The Sun Also Rises\".\n",
            "Table 3: Examples from generation tasks. RAG models generate more speciﬁc and factually accurate\n",
            "responses\n"
          ]
        }
      ],
      "source": [
        "parentDocs = ParentDocumentRetriever(\n",
        "    ranked_docs=windowDocs,\n",
        "    all_chunks=chunks\n",
        ")\n",
        "print(len(parentDocs))\n",
        "print(\"Windowed length:\", len(windowDocs[0].page_content))\n",
        "print(\"Parent length:\", len(parentDocs[0].page_content))\n",
        "print()\n",
        "print(\"PARENT DOC SAMPLE:\")\n",
        "print()\n",
        "print(parentDocs[0].page_content[:1000])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fltyJZfnZSYs",
        "outputId": "13326e95-237c-49d2-f987-21b87380cdbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Windowed chunk pages: [6, 7, 18, 1, 4]\n",
            "Parent doc pages: [6, 7, 18, 1, 4]\n"
          ]
        }
      ],
      "source": [
        "print(\"Windowed chunk pages:\", [doc.metadata[\"page\"] for doc in windowDocs])\n",
        "print(\"Parent doc pages:\", [doc.metadata[\"page\"] for doc in parentDocs])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU4sVJbD8ZDH"
      },
      "source": [
        "### STEP 9 (CONTEXT COMPRESSION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "_DMG1gV68cxj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "def ContextualCompression(\n",
        "    query,\n",
        "    docs,\n",
        "    embeddings,\n",
        "    similarity_threshold=0.45\n",
        "):\n",
        "    queryEmbedding = embeddings.embed_query(query)\n",
        "    compressedDocs = []\n",
        "\n",
        "    for doc in docs:\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', doc.page_content)\n",
        "        keptSentences = []\n",
        "\n",
        "        for sent in sentences:\n",
        "            sent_embedding = embeddings.embed_query(sent)\n",
        "            similarity = np.dot(queryEmbedding, sent_embedding) / (\n",
        "                np.linalg.norm(queryEmbedding) * np.linalg.norm(sent_embedding)\n",
        "            )\n",
        "\n",
        "            if similarity >= similarity_threshold:\n",
        "                keptSentences.append(sent)\n",
        "\n",
        "        if keptSentences:\n",
        "            compressedDocs.append(\n",
        "                Document(\n",
        "                    page_content=\" \".join(keptSentences),\n",
        "                    metadata=doc.metadata\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            compressedDocs.append(doc)\n",
        "\n",
        "    return compressedDocs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCe0iKPMfBPt",
        "outputId": "bf1ac84c-f2ff-4d31-e175-52b7e92b2235"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5\n",
            "The posterior for document 1 is high ingway\" for Jeopardy generation with 5 retrieved documents. 4.5 Additional Results\n",
            "Generation Diversity Section 4.3 shows that RAG models are more factual and speciﬁc than\n",
            "BART for Jeopardy question generation. Following recent work on diversity-promoting decoding BART for Jeopardy question generation.\n"
          ]
        }
      ],
      "source": [
        "compressedDocs = ContextualCompression(\n",
        "    query=question,\n",
        "    docs=parentDocs,\n",
        "    embeddings=embedding\n",
        ")\n",
        "\n",
        "\n",
        "print(len(compressedDocs))\n",
        "print(compressedDocs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MbqkSJVVU2C"
      },
      "source": [
        "### STEP 10 (LongContextReorder) For Solving Lost in Middle Phenomenon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "MjLT_MQtGfkR"
      },
      "outputs": [],
      "source": [
        "finalDocs = compressedDocs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtYE3ZNZVF1y",
        "outputId": "cc4a618e-1162-4241-ee3b-f39770da9d6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1) --> The posterior for document 1 is high ingway\" for Jeopardy generation with 5 retrieved documents. 4.5 Additional Results\n",
            "Generation Diversity Section 4.3 shows that RAG models are more factual and speciﬁc than\n",
            "BART for Jeopardy question generation. Following recent work on diversity-promoting decoding BART for Jeopardy question generation.\n",
            "(2) --> Table 7: Number of instances in the datasets used. *A hidden subset of this data is used for evaluation\n",
            "Task Train Development Test\n",
            "Natural Questions 79169 8758 3611\n",
            "TriviaQA 78786 8838 11314\n",
            "WebQuestions 3418 362 2033\n",
            "CuratedTrec 635 134 635\n",
            "Jeopardy Question Generation 97392 13714 26849\n",
            "MS-MARCO 153726 12468 101093*\n",
            "FEVER-3-way 145450 10000 10000\n",
            "FEVER-2-way 96966 6666 6666 Jeopardy Question Generation 97392 13714 26849\n",
            "MS-MARCO 153726 12468 101093*\n",
            "FEVER-3-way 145450 10000 10000\n",
            "FEVER-2-way 96966 6666 6666\n",
            "parameters. The best performing \"closed-book\" (parametric only) open-domain QA model is T5-11B\n",
            "with 11 Billion trainable parameters. The T5 model with the closest number of parameters to our with 11 Billion trainable parameters. The T5 model with the closest number of parameters to our\n",
            "models is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [52],\n",
            "substantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non-\n",
            "parametric models require far fewer trainable parameters for strong open-domain QA performance. parametric models require far fewer trainable parameters for strong open-domain QA performance.\n",
            "The non-parametric memory index does not consist of trainable parameters, but does consists of 21M\n",
            "728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit ﬂoating\n",
            "point precision to manage memory and disk footprints.\n",
            "H Retrieval Collapse point precision to manage memory and disk footprints.\n",
            "H Retrieval Collapse\n",
            "In preliminary experiments, we observed that for some tasks such as story generation [ 11], the\n",
            "retrieval component would “collapse” and learn to retrieve the same documents regardless of the\n",
            "input. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents, input. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents,\n",
            "and the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit\n",
            "requirement for factual knowledge in some tasks, or the longer target sequences, which could result requirement for factual knowledge in some tasks, or the longer target sequences, which could result\n",
            "in less informative gradients for the retriever. Perez et al.[46] also found spurious retrieval results\n",
            "when optimizing a retrieval component in order to improve performance on downstream tasks.\n",
            "I Number of instances per dataset when optimizing a retrieval component in order to improve performance on downstream tasks.\n",
            "I Number of instances per dataset\n",
            "The number of training, development and test datapoints in each of our datasets is shown in Table 7.\n",
            "19\n",
            "(3) --> 3.3 Jeopardy Question Generation\n",
            "To evaluate RAG’s generation abilities in a non-QA setting, we study open-domain question gen-\n",
            "eration. Rather than use questions from standard open-domain QA tasks, which typically consist\n",
            "of short, simple questions, we propose the more demanding task of generating Jeopardy questions. Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity. For example, “The World Cup” is the answer to the question “In 1986 Mexico scored as the ﬁrst For example, “The World Cup” is the answer to the question “In 1986 Mexico scored as the ﬁrst\n",
            "country to host this international sports competition twice.” As Jeopardy questions are precise,\n",
            "factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a\n",
            "challenging knowledge-intensive generation task. factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a\n",
            "challenging knowledge-intensive generation task.\n",
            "(4) --> (y)\n",
            "Question Answering:\n",
            "Answer GenerationRetriever pη\n",
            "(Non-Parametric)\n",
            "z4\n",
            "z3\n",
            "z2\n",
            "z1\n",
            "d(z)\n",
            "Jeopardy Question\n",
            "Generation:\n",
            "Answer Query Question Answering:\n",
            "Answer GenerationRetriever pη\n",
            "(Non-Parametric)\n",
            "z4\n",
            "z3\n",
            "z2\n",
            "z1\n",
            "d(z)\n",
            "Jeopardy Question\n",
            "Generation:\n",
            "Answer Query\n",
            "Figure 1: Overview of our approach. For knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question\n",
            "generation, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and generation, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and\n",
            "diverse than a BART baseline.\n",
            "(5) --> Table 4: Human assessments for the Jeopardy\n",
            "Question Generation Task.\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_transformers import LongContextReorder\n",
        "reorder = LongContextReorder()\n",
        "finalDocsReordered = reorder.transform_documents(finalDocs)\n",
        "\n",
        "for i, doc in enumerate(finalDocsReordered):\n",
        "    print(f\"({i+1}) --> {doc.page_content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KQqnM-TXxwT"
      },
      "source": [
        "### STEP 9 (AUGMENTATION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "M1ocMA9GXaRB",
        "outputId": "f9928fe1-94d3-4616-eaec-bbab3e1e3870"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The posterior for document 1 is high ingway\" for Jeopardy generation with 5 retrieved documents. 4.5 Additional Results\\nGeneration Diversity Section 4.3 shows that RAG models are more factual and speciﬁc than\\nBART for Jeopardy question generation. Following recent work on diversity-promoting decoding BART for Jeopardy question generation.\\n\\n---\\n\\nTable 7: Number of instances in the datasets used. *A hidden subset of this data is used for evaluation\\nTask Train Development Test\\nNatural Questions 79169 8758 3611\\nTriviaQA 78786 8838 11314\\nWebQuestions 3418 362 2033\\nCuratedTrec 635 134 635\\nJeopardy Question Generation 97392 13714 26849\\nMS-MARCO 153726 12468 101093*\\nFEVER-3-way 145450 10000 10000\\nFEVER-2-way 96966 6666 6666 Jeopardy Question Generation 97392 13714 26849\\nMS-MARCO 153726 12468 101093*\\nFEVER-3-way 145450 10000 10000\\nFEVER-2-way 96966 6666 6666\\nparameters. The best performing \"closed-book\" (parametric only) open-domain QA model is T5-11B\\nwith 11 Billion trainable parameters. The T5 model with the closest number of parameters to our with 11 Billion trainable parameters. The T5 model with the closest number of parameters to our\\nmodels is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [52],\\nsubstantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/non-\\nparametric models require far fewer trainable parameters for strong open-domain QA performance. parametric models require far fewer trainable parameters for strong open-domain QA performance.\\nThe non-parametric memory index does not consist of trainable parameters, but does consists of 21M\\n728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit ﬂoating\\npoint precision to manage memory and disk footprints.\\nH Retrieval Collapse point precision to manage memory and disk footprints.\\nH Retrieval Collapse\\nIn preliminary experiments, we observed that for some tasks such as story generation [ 11], the\\nretrieval component would “collapse” and learn to retrieve the same documents regardless of the\\ninput. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents, input. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents,\\nand the RAG model would perform equivalently to BART. The collapse could be due to a less-explicit\\nrequirement for factual knowledge in some tasks, or the longer target sequences, which could result requirement for factual knowledge in some tasks, or the longer target sequences, which could result\\nin less informative gradients for the retriever. Perez et al.[46] also found spurious retrieval results\\nwhen optimizing a retrieval component in order to improve performance on downstream tasks.\\nI Number of instances per dataset when optimizing a retrieval component in order to improve performance on downstream tasks.\\nI Number of instances per dataset\\nThe number of training, development and test datapoints in each of our datasets is shown in Table 7.\\n19\\n\\n---\\n\\n3.3 Jeopardy Question Generation\\nTo evaluate RAG’s generation abilities in a non-QA setting, we study open-domain question gen-\\neration. Rather than use questions from standard open-domain QA tasks, which typically consist\\nof short, simple questions, we propose the more demanding task of generating Jeopardy questions. Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity. For example, “The World Cup” is the answer to the question “In 1986 Mexico scored as the ﬁrst For example, “The World Cup” is the answer to the question “In 1986 Mexico scored as the ﬁrst\\ncountry to host this international sports competition twice.” As Jeopardy questions are precise,\\nfactual statements, generating Jeopardy questions conditioned on their answer entities constitutes a\\nchallenging knowledge-intensive generation task. factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a\\nchallenging knowledge-intensive generation task.\\n\\n---\\n\\n(y)\\nQuestion Answering:\\nAnswer GenerationRetriever pη\\n(Non-Parametric)\\nz4\\nz3\\nz2\\nz1\\nd(z)\\nJeopardy Question\\nGeneration:\\nAnswer Query Question Answering:\\nAnswer GenerationRetriever pη\\n(Non-Parametric)\\nz4\\nz3\\nz2\\nz1\\nd(z)\\nJeopardy Question\\nGeneration:\\nAnswer Query\\nFigure 1: Overview of our approach. For knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question\\ngeneration, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and generation, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and\\ndiverse than a BART baseline.\\n\\n---\\n\\nTable 4: Human assessments for the Jeopardy\\nQuestion Generation Task.'"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def buildContext(docs):\n",
        "    return \"\\n\\n---\\n\\n\".join(doc.page_content for doc in docs)\n",
        "context = buildContext(finalDocsReordered)\n",
        "context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP2cCOaOJ7Xx"
      },
      "source": [
        "### STEP 8 (GENERATION)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "6oZWrGUwgf7B"
      },
      "outputs": [],
      "source": [
        "def RetrieveContext(question):\n",
        "    # Self-Query Analyzer\n",
        "\n",
        "    semanticQuery, filters = selfQueryAnalyzer(question)\n",
        "\n",
        "    # HyDE (Dense Recall)\n",
        "    hydeDoc = GenerateHyDE(semanticQuery)\n",
        "\n",
        "    # Merger Retriever\n",
        "    denseDocs = denseRetriever.invoke(hydeDoc)\n",
        "    sparseDocs = sparseRetriever.invoke(semanticQuery)\n",
        "\n",
        "    if filters:\n",
        "        metadataDocs = [\n",
        "            doc for doc in chunks\n",
        "            if all(doc.metadata.get(k) == v for k, v in filters.items())\n",
        "        ]\n",
        "    else:\n",
        "        metadataDocs = []\n",
        "\n",
        "    def dedup_docs(docs):\n",
        "        seen = set()\n",
        "        unique = []\n",
        "        for d in docs:\n",
        "            key = (d.page_content, tuple(sorted(d.metadata.items())))\n",
        "            if key not in seen:\n",
        "                seen.add(key)\n",
        "                unique.append(d)\n",
        "        return unique\n",
        "\n",
        "    mergedDocs = dedup_docs(denseDocs + sparseDocs + metadataDocs)\n",
        "\n",
        "    # Flash Reranking\n",
        "    passages = [\n",
        "        {\"id\": i, \"text\": doc.page_content}\n",
        "        for i, doc in enumerate(mergedDocs)\n",
        "    ]\n",
        "\n",
        "    rerankingResults = reranking(\n",
        "        query=semanticQuery,\n",
        "        passages=passages,\n",
        "        choice=\"Medium\"\n",
        "    )\n",
        "\n",
        "    flashDocs = [\n",
        "        mergedDocs[item[\"id\"]]\n",
        "        for item in rerankingResults[:20]\n",
        "    ]\n",
        "\n",
        "    # Cross-Encoder\n",
        "    pairs = [[semanticQuery, doc.page_content] for doc in flashDocs]\n",
        "    scores = crossEncoder.predict(pairs)\n",
        "\n",
        "    crossReranked = sorted(zip(scores, flashDocs), reverse=True)\n",
        "    top_chunks = [doc for _, doc in crossReranked[:5]]\n",
        "\n",
        "\n",
        "    # Sentence Window\n",
        "    windowDocs = SentenceWindowRetriever(\n",
        "        rankedChunks=top_chunks,\n",
        "        sentenceIndex=sentenceIndex,\n",
        "        windowSize=2\n",
        "    )\n",
        "\n",
        "    # Parent Document\n",
        "    parentDocs = ParentDocumentRetriever(\n",
        "        ranked_docs=windowDocs,\n",
        "        all_chunks=chunks\n",
        "    )\n",
        "\n",
        "    # Contextual Compression\n",
        "    compressedDocs = ContextualCompression(\n",
        "        query=semanticQuery,\n",
        "        docs=parentDocs,\n",
        "        embeddings=embedding,\n",
        "        similarity_threshold=0.45\n",
        "    )\n",
        "\n",
        "    # Long Context Reorder\n",
        "    finalDocsReordered = reorder.transform_documents(compressedDocs)\n",
        "\n",
        "    # Augmentation\n",
        "    context = buildContext(finalDocsReordered)\n",
        "\n",
        "    return context\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "5d_PHnjeYDBa"
      },
      "outputs": [],
      "source": [
        "def generateResponse(question):\n",
        "    context = RetrieveContext(question)\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"\"\"You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "Use ten sentences maximum and keep the answer concise.\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Question: {question}\n",
        "Answer:\n",
        "\"\"\",\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "\n",
        "    chain = prompt | model | parser\n",
        "    return chain.invoke({\"context\": context, \"question\": question})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GflT-Zu4bGyK",
        "outputId": "80ac2179-f6aa-41bb-c38c-c9605d6925a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your question related to the document: What is index hot swapping discussed on page 7,8?\n",
            "\n",
            "Generating answer...\n",
            "\n",
            "Answer: Index hot-swapping is discussed on page 7 and 8 as an advantage of non-parametric memory models like RAG, where knowledge can be easily updated at test time without requiring any retraining.\n",
            "Do you want to ask another question? (yes/no): yes \n",
            "Enter your question related to the document: Who is virat kohli?\n",
            "\n",
            "Generating answer...\n",
            "\n",
            "Answer: I don't know who Siraj Raval is, but I do know who Virat Kohli is. Unfortunately, I don't have any information about Virat Kohli in the provided context.\n",
            "Do you want to ask another question? (yes/no): yes\n",
            "Enter your question related to the document: what is effect of Effect of Retrieving more documents?\n",
            "\n",
            "Generating answer...\n",
            "\n",
            "Answer: Retrieving more documents at test time can improve Open-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved documents.\n",
            "Do you want to ask another question? (yes/no): no\n",
            "\n",
            "Thank you for using the document Q&A system. Goodbye!\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    question = input(\"Enter your question related to the document: \")\n",
        "\n",
        "    if not question.strip():\n",
        "        print(\"Please enter a valid question.\\n\")\n",
        "        continue\n",
        "\n",
        "    print(\"\\nGenerating answer...\\n\")\n",
        "    answer = generateResponse(question)\n",
        "    print(\"Answer:\", answer)\n",
        "\n",
        "    continue_chat = input(\"Do you want to ask another question? (yes/no): \").strip().lower()\n",
        "    if continue_chat not in ['yes', 'y']:\n",
        "        print(\"\\nThank you for using the document Q&A system. Goodbye!\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiErEnCcGSPH",
        "outputId": "117044af-e4bd-4564-9c76-24444031ddc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your question related to the document: What is index hot swapping discussed on page 7,8?\n",
            "\n",
            "Generating answer...\n",
            "\n",
            "Answer: Index hot-swapping is discussed as an advantage of non-parametric memory models like RAG, allowing knowledge to be easily updated at test time.\n",
            "Do you want to ask another question? (yes/no): yes\n",
            "Enter your question related to the document:  Who is virat kohli?\n",
            "\n",
            "Generating answer...\n",
            "\n",
            "Answer: I don't know who Virat Kohli is.\n",
            "Do you want to ask another question? (yes/no): yes\n",
            "Enter your question related to the document: what is effect of Effect of Retrieving more documents?\n",
            "\n",
            "Generating answer...\n",
            "\n",
            "Answer: Retrieving more documents at test time can monotonically improve Open-domain QA results for RAG-Sequence, but performance peaks at 10 retrieved documents. For RAG-Token, performance peaks at 10 retrieved documents. Retrieving more documents also leads to higher Rouge-L scores for RAG-Token at the expense of Bleu-1.\n",
            "Do you want to ask another question? (yes/no): no\n",
            "\n",
            "Thank you for using the document Q&A system. Goodbye!\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    question = input(\"Enter your question related to the document: \")\n",
        "\n",
        "    if not question.strip():\n",
        "        print(\"Please enter a valid question.\\n\")\n",
        "        continue\n",
        "\n",
        "    print(\"\\nGenerating answer...\\n\")\n",
        "    answer = generateResponse(question)\n",
        "    print(\"Answer:\", answer)\n",
        "\n",
        "    continue_chat = input(\"Do you want to ask another question? (yes/no): \").strip().lower()\n",
        "    if continue_chat not in ['yes', 'y']:\n",
        "        print(\"\\nThank you for using the document Q&A system. Goodbye!\")\n",
        "        break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
