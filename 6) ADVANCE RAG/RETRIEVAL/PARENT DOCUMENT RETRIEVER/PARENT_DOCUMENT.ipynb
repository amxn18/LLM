{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhXlPLZl0mAw"
      },
      "outputs": [],
      "source": [
        "!pip install flashrank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SqZM5P2J3sv0"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-community langchain-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNyYRq_O4RmR"
      },
      "outputs": [],
      "source": [
        "!pip install pypdf tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvattsXe5hBO"
      },
      "outputs": [],
      "source": [
        "%pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30ZCRxAl6jSY",
        "outputId": "576b0190-d738-4260-cfb4-631512c96bbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.12/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rank_bm25) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4p_dMaz5h-Bd"
      },
      "outputs": [],
      "source": [
        "%pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K-ambBG-yB4"
      },
      "source": [
        "## STEP 1 (DATA INGESTION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmwFTQYL1fIm"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nYj0oAJM3fOP"
      },
      "outputs": [],
      "source": [
        "filePath = \"/content/nlp.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MAZEiz-335jf"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0l-qRvNH3oUF"
      },
      "outputs": [],
      "source": [
        "def loadDocs(path:str):\n",
        "  loader = PyPDFLoader(path)\n",
        "  docs = loader.load()\n",
        "  return docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dCaO98l4LFV",
        "outputId": "d7410217-108b-4bfb-f365-3d0e34062a23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "19\n",
            "page_content='Retrieval-Augmented Generation for\n",
            "Knowledge-Intensive NLP Tasks\n",
            "Patrick Lewis†‡, Ethan Perez⋆,\n",
            "Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\n",
            "Mike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†\n",
            "†Facebook AI Research;‡University College London;⋆New York University;\n",
            "plewis@fb.com\n",
            "Abstract\n",
            "Large pre-trained language models have been shown to store factual knowledge\n",
            "in their parameters, and achieve state-of-the-art results when ﬁne-tuned on down-\n",
            "stream NLP tasks. However, their ability to access and precisely manipulate knowl-\n",
            "edge is still limited, and hence on knowledge-intensive tasks, their performance\n",
            "lags behind task-speciﬁc architectures. Additionally, providing provenance for their\n",
            "decisions and updating their world knowledge remain open research problems. Pre-\n",
            "trained models with a differentiable access mechanism to explicit non-parametric\n",
            "memory have so far been only investigated for extractive downstream tasks. We\n",
            "explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation\n",
            "(RAG) — models which combine pre-trained parametric and non-parametric mem-\n",
            "ory for language generation. We introduce RAG models where the parametric\n",
            "memory is a pre-trained seq2seq model and the non-parametric memory is a dense\n",
            "vector index of Wikipedia, accessed with a pre-trained neural retriever. We com-\n",
            "pare two RAG formulations, one which conditions on the same retrieved passages\n",
            "across the whole generated sequence, and another which can use different passages\n",
            "per token. We ﬁne-tune and evaluate our models on a wide range of knowledge-\n",
            "intensive NLP tasks and set the state of the art on three open domain QA tasks,\n",
            "outperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract\n",
            "architectures. For language generation tasks, we ﬁnd that RAG models generate\n",
            "more speciﬁc, diverse and factual language than a state-of-the-art parametric-only\n",
            "seq2seq baseline.\n",
            "1 Introduction\n",
            "Pre-trained neural language models have been shown to learn a substantial amount of in-depth knowl-\n",
            "edge from data [47]. They can do so without any access to an external memory, as a parameterized\n",
            "implicit knowledge base [51, 52]. While this development is exciting, such models do have down-\n",
            "sides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into\n",
            "their predictions, and may produce “hallucinations” [ 38]. Hybrid models that combine parametric\n",
            "memory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these\n",
            "issues because knowledge can be directly revised and expanded, and accessed knowledge can be\n",
            "inspected and interpreted. REALM [ 20] and ORQA [ 31], two recently introduced models that\n",
            "combine masked language models [8] with a differentiable retriever, have shown promising results,\n",
            "arXiv:2005.11401v4  [cs.CL]  12 Apr 2021' metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/nlp.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1'}\n"
          ]
        }
      ],
      "source": [
        "docs = loadDocs(filePath)\n",
        "print(len(docs))\n",
        "print(docs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NC-DK0xR4cyY"
      },
      "outputs": [],
      "source": [
        "def splitDocs(docs):\n",
        "  splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size = 400,\n",
        "      chunk_overlap = 150\n",
        "  )\n",
        "  chunks = splitter.split_documents(docs)\n",
        "  return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZk4u8Lg4uqg",
        "outputId": "49b9ada4-57d8-43e7-d19f-4cb7881f68e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "262\n",
            "page_content='Retrieval-Augmented Generation for\n",
            "Knowledge-Intensive NLP Tasks\n",
            "Patrick Lewis†‡, Ethan Perez⋆,\n",
            "Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\n",
            "Mike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†\n",
            "†Facebook AI Research;‡University College London;⋆New York University;\n",
            "plewis@fb.com\n",
            "Abstract' metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/nlp.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1'}\n"
          ]
        }
      ],
      "source": [
        "chunks = splitDocs(docs)\n",
        "print(len(chunks))\n",
        "print(chunks[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qMdyDfyU4gKO"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings, ChatHuggingFace, HuggingFaceEndpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_UYWfqZ5Pas"
      },
      "outputs": [],
      "source": [
        "# HF API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChqbGWVt5BC6"
      },
      "outputs": [],
      "source": [
        "embedding = HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "UU7S5k-b60Mb"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace, HuggingFacePipeline\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id = \"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    task = \"text-generation\",\n",
        ")\n",
        "model = ChatHuggingFace(llm = llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ijUsbmxD95ia"
      },
      "outputs": [],
      "source": [
        "parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5RNDoLmRgGC"
      },
      "source": [
        "### STEP 2 (SELF QUERYING ANALYZER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "qii99YBVRm85"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "import json\n",
        "selfQueryPrompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "You are a query analyzer.\n",
        "\n",
        "Your task is to split the question into:\n",
        "1. semantic_query → what should be searched semantically\n",
        "2. filters → structured metadata constraints\n",
        "\n",
        "Allowed filters:\n",
        "- page (integer)\n",
        "\n",
        "RULES:\n",
        "- If the question mentions \"page X\", extract page = X\n",
        "- Remove filter-related words from the semantic query\n",
        "- If no filters apply, return an empty filters object\n",
        "\n",
        "EXAMPLES:\n",
        "\n",
        "Question:\n",
        "\"What is discussed on page 6 about Jeopardy Question Generation?\"\n",
        "\n",
        "Output:\n",
        "{{\n",
        "  \"semantic_query\": \"Jeopardy Question Generation\",\n",
        "  \"filters\": {{ \"page\": 6 }}\n",
        "}}\n",
        "\n",
        "Question:\n",
        "\"Explain Jeopardy Question Generation\"\n",
        "\n",
        "Output:\n",
        "{{\n",
        "  \"semantic_query\": \"Jeopardy Question Generation\",\n",
        "  \"filters\": {{}}\n",
        "}}\n",
        "\n",
        "NOW ANALYZE THIS QUESTION:\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Return ONLY valid JSON.\n",
        "\"\"\",\n",
        "    input_variables=[\"question\"]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "rkSVlzJbTD39"
      },
      "outputs": [],
      "source": [
        "def selfQueryAnalyzer(question):\n",
        "    chain = selfQueryPrompt | model | parser\n",
        "    response = chain.invoke({\"question\": question})\n",
        "\n",
        "    try:\n",
        "        parsed = json.loads(response)\n",
        "        semanticQuery = parsed.get(\"semantic_query\", question)\n",
        "        filters = parsed.get(\"filters\", {})\n",
        "    except Exception as e:\n",
        "        print(\"Parsing failed:\", e)\n",
        "        semanticQuery = question\n",
        "        filters = {}\n",
        "\n",
        "    return semanticQuery, filters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0P35Zd2ITyjP",
        "outputId": "b8da86c2-74b3-40d4-8d31-74c22ec1686b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Semantic Query: models\n",
            "Filters: {'page': '3,4'}\n"
          ]
        }
      ],
      "source": [
        "q1 = \"What is discussed on page 6 about Jeopardy Question Generation?\"\n",
        "q2 = \"Explain Jeopardy Question Generation\"\n",
        "q3 = \"What are the results in Table 2?\"\n",
        "q4 = \"What are the models discussed on page no 3,4 of this research paper?\"\n",
        "semantic_query, filters = selfQueryAnalyzer(q4)\n",
        "\n",
        "print(\"Semantic Query:\", semantic_query)\n",
        "print(\"Filters:\", filters)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiPN3YMp--yq"
      },
      "source": [
        "## STEP 3 (HYBRID SEARCH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "oqhFP3DYymhs"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "vectorStore = Chroma.from_documents(chunks, embedding)\n",
        "similarityRetriever = vectorStore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 20})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "MLeX7Jqyyppd"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "keywordRetriever = BM25Retriever.from_documents(chunks)\n",
        "keywordRetriever.k = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ocka7mc16ucT"
      },
      "outputs": [],
      "source": [
        "def hybridRetriever(\n",
        "    query,\n",
        "    denseRetriever,\n",
        "    sparseRetriever,\n",
        "    filters=None,\n",
        "    denseWeight=0.5,\n",
        "    sparseWeight=0.5,\n",
        "    rrf_k=60\n",
        "):\n",
        "    scores = {}\n",
        "    doc_map = {}\n",
        "\n",
        "    denseDocs = denseRetriever.invoke(query)\n",
        "    for rank, doc in enumerate(denseDocs):\n",
        "        key = doc.page_content\n",
        "        doc_map[key] = doc\n",
        "        scores[key] = scores.get(key, 0) + denseWeight / (rank + 1 + rrf_k)\n",
        "\n",
        "    sparseDocs = sparseRetriever.invoke(query)\n",
        "    for rank, doc in enumerate(sparseDocs):\n",
        "        key = doc.page_content\n",
        "        doc_map[key] = doc\n",
        "        scores[key] = scores.get(key, 0) + sparseWeight / (rank + 1 + rrf_k)\n",
        "\n",
        "    ranked_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    retrieved_docs = [doc_map[content] for content, _ in ranked_docs]\n",
        "\n",
        "    # Metadata filtering\n",
        "    if filters:\n",
        "        filtered_docs = []\n",
        "        for doc in retrieved_docs:\n",
        "            keep = True\n",
        "            for key, value in filters.items():\n",
        "                if doc.metadata.get(key) != value:\n",
        "                    keep = False\n",
        "                    break\n",
        "            if keep:\n",
        "                filtered_docs.append(doc)\n",
        "\n",
        "        if filtered_docs:\n",
        "            return filtered_docs\n",
        "\n",
        "    return retrieved_docs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jbVeXAXZSbM",
        "outputId": "0cdbd5e9-3ea4-4711-fb1a-6204ef6e4331"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "37\n",
            "page_content='can be ﬁne-tuned for strong performance on a variety of tasks.\n",
            "Memory-based Architectures Our document index can be seen as a large external memory for\n",
            "neural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns\n",
            "to retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our' metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/nlp.pdf', 'total_pages': 19, 'page': 8, 'page_label': '9'}\n"
          ]
        }
      ],
      "source": [
        "question = \"What is mentioned about Memory-based Architectures on page 9?\"\n",
        "semantic_query, filters = selfQueryAnalyzer(question)\n",
        "\n",
        "hybridResults = hybridRetriever(\n",
        "    query=semantic_query,\n",
        "    denseRetriever=similarityRetriever,\n",
        "    sparseRetriever=keywordRetriever,\n",
        "    filters=filters\n",
        ")\n",
        "\n",
        "print(len(hybridResults))\n",
        "print(hybridResults[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC4IeChO_E-I"
      },
      "source": [
        "## STEP 4 (FLASH RERANKING)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqgHbk4yRJSj"
      },
      "outputs": [],
      "source": [
        "!pip install flashrank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "TbAHbohsRQWE"
      },
      "outputs": [],
      "source": [
        "from flashrank.Ranker import Ranker, RerankRequest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "rjp06GhVbXY4"
      },
      "outputs": [],
      "source": [
        "from flashrank.Ranker import Ranker, RerankRequest\n",
        "def reranking(query, passages, choice):\n",
        "    if choice == \"Nano\":\n",
        "        ranker = Ranker()\n",
        "    elif choice == \"Small\":\n",
        "        ranker = Ranker(model_name=\"ms-marco-MiniLM-L-12-v2\", cache_dir=\"/opt\")\n",
        "    elif choice == \"Medium\":\n",
        "        ranker = Ranker(model_name=\"rank-T5-flan\", cache_dir=\"/opt\")\n",
        "    elif choice == \"Large\":\n",
        "        ranker = Ranker(model_name=\"ms-marco-MultiBERT-L-12\", cache_dir=\"/opt\")\n",
        "\n",
        "    rerankRequest = RerankRequest(\n",
        "        query=query,\n",
        "        passages=passages\n",
        "    )\n",
        "\n",
        "    results = ranker.rerank(rerankRequest)\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5aDZdg0bhnp",
        "outputId": "3cf0e022-0cc1-42ba-cd61-04a3e36311a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': 0,\n",
              " 'text': 'can be ﬁne-tuned for strong performance on a variety of tasks.\\nMemory-based Architectures Our document index can be seen as a large external memory for\\nneural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns\\nto retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our'}"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "passages = [\n",
        "    {\"id\": i, \"text\": doc.page_content}\n",
        "    for i, doc in enumerate(hybridResults)\n",
        "]\n",
        "passages[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChIouBGJbodo",
        "outputId": "a4644308-bb84-40d3-c3e7-3438f3dd138b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "rank-T5-flan.zip: 100%|██████████| 73.7M/73.7M [00:01<00:00, 48.1MiB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "37\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'id': 33,\n",
              " 'text': 'distributed representations, which makes the memory both (i) human-readable, lending a form of\\ninterpretability to our model, and (ii) human-writable, enabling us to dynamically update the model’s\\nmemory by editing the document index. This approach has also been used in knowledge-intensive\\ndialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF',\n",
              " 'score': np.float32(0.650604)}"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rerankingResults = reranking(\n",
        "    query=semantic_query,\n",
        "    passages=passages,\n",
        "    choice=\"Medium\"\n",
        ")\n",
        "print(len(rerankingResults))\n",
        "rerankingResults[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PwJa5yPSYDf"
      },
      "source": [
        "## STEP 5 (CROSS-ENCODERS) RERANKING AGAIN FOR MORE ACCURATE RESULTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CHF8IJl6uxm",
        "outputId": "5d403d49-a8f5-4c15-f925-d01dffb925a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page_content='distributed representations, which makes the memory both (i) human-readable, lending a form of\n",
            "interpretability to our model, and (ii) human-writable, enabling us to dynamically update the model’s\n",
            "memory by editing the document index. This approach has also been used in knowledge-intensive\n",
            "dialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF' metadata={'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'page': 8, 'producer': 'pdfTeX-1.40.21', 'creationdate': '2021-04-13T00:48:38+00:00', 'subject': '', 'title': '', 'keywords': '', 'trapped': '/False', 'source': '/content/nlp.pdf', 'moddate': '2021-04-13T00:48:38+00:00', 'creator': 'LaTeX with hyperref', 'total_pages': 19, 'page_label': '9', 'author': ''}\n"
          ]
        }
      ],
      "source": [
        "flashDocs = [hybridResults[item[\"id\"]] for item in rerankingResults]\n",
        "print(flashDocs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395,
          "referenced_widgets": [
            "78486a6464094cce85909900641547cc",
            "b4d3d316c11a4ad9841936fcb937a0f3",
            "e8b0e00b843745fcae8385ed8fa6e7db",
            "92847193ca444b75a1fa65da2c95d163",
            "0584c93bc37f40518f459e266ad17643",
            "f01ef53817a24c60985802332ab3129f",
            "91e6079fa20f40d1ad5344434bba3569",
            "f4b65389dd454f8296576e74a737b202",
            "9eb68df42bef49739c0cebd0acedbfe8",
            "e4a41de76be244ee8fc97b11591b5d70",
            "c97dd0bf2724438a9747ae99a12ec7e7",
            "9428279acdbe4583844e5dc0be8a3919",
            "82e64c28fe8148ebb8bade075a27fa8b",
            "ee36cf04b30e44cf9b2ff4cabfb53411",
            "617490a015c64896836ce657faf7c699",
            "b87c900e25eb425497a445cceb26bcfc",
            "600fc3e89ad64c208e498f128f20f1cf",
            "ea518602afb74d0ca4cd9415f6a6c3b5",
            "e47f93b5ac9843a5a201b417a14bc409",
            "34d12be3686d41f5ae75cf763b05e710",
            "17106d45bec64fac8db1376668008fa8",
            "d8cfccf266904d53b561b97d7315fbc8",
            "8e6cc2d9714749ed9053c2c9745498b5",
            "33965e617c4f4786ad7f9a800b991160",
            "8ea1b19cdf114537ba2877e0f05e3f22",
            "842a9a1e6cb440a59a8759a75f2c87af",
            "badfbd7c811048e294bfe0a5b1b76978",
            "568b38120ce44c2d868c2565e375d9d9",
            "ee7323ab08c543989680bcd7657cf364",
            "464aee6e547146a9abf4da9aa851546b",
            "f4fa3831fed145178dda79a5fcabf680",
            "091d1fd82d28467cab815d7d504c0837",
            "6afcf644bac3417eab3291cbb2f12770",
            "209aef5c7ab0404187743d7af0901c82",
            "bd22172b7e21493d8847697269b7abcb",
            "88db619dc0ce4e96ab0f4b86371c0bfe",
            "119e8787f3754b0a891d7c075b929faf",
            "9c591210e8db49e58d660ee9bcdf835e",
            "d93a3aae385343e696d9c4eb2390a255",
            "46f2d40b85694d7e85e1acdaff72503f",
            "3fb74b1293f14d64bf30440aa8fe0fd4",
            "d11d95c90c9046fe98410bdba570fd6a",
            "69507e8673d44709821ca1bfbf4948b3",
            "2ae974b37c5b483b8cc9deab25f58f45",
            "ad333bec169f44a8a8ace28117147915",
            "3c6db7056636491eae0dc5a55a62b7f1",
            "8a7ce24c3e2a4ed596b7b47dd24ce2f0",
            "6653ac3353214f72b2960c5f35f2ff0f",
            "db12be3f6c274514a1c8e906bac0ff5e",
            "3a5e6ed3bfad4b7680f0c813c494e993",
            "736da3974a5741bf8dc0a5f2d23b49fe",
            "643f692a3dec46e8b59aa836c0be64b7",
            "cec374069bd54cfd90a689e2de063001",
            "e5c7e0cfd1b3469788f6070969c63307",
            "414b24d9b9c547d0bc85d83c273f2289",
            "e134cf3668974876940fa0a92d2bf51b",
            "36b77d9aa08c4c158af684c30a59d5fa",
            "1da63733b0f646df994622ec88618387",
            "f320fd9d32014cc59b5e4fbab4c00406",
            "49959ab7fc464bda818716fee48c26b8",
            "7bbe9edbf4394775aea60982ff3ac078",
            "6f1b13346f8e46db9494707af2d96d7a",
            "ac21f68879174119bc4a6a6a982ed03a",
            "205a71b3766940379f429ce49db3d06e",
            "2e30c800c8204f48ac3c76eb8a626556",
            "9f1cf124f77544f3be7d0a0ae32ed012",
            "a3345449b54a4615a447c8e3ec5945eb",
            "f9e3fc4e3e9e4a9d918dc71b6a897f16",
            "12f80dec66914f73a7193348432f74b2",
            "0874169bb9544b4d8b4f085b559775e6",
            "9a4f54f7043d44439e1ee6bc434b34bc",
            "051e76ffe7954469afd28aaf432bc3c7",
            "01ed3d92d4ab423c8a01b8fef6dc5923",
            "6a7264630e7e49858dfcc103471e94e5",
            "70e93518b1e841c1810f49be2456197c",
            "9bba07d3d8084cd9a3966bde7978a21e",
            "79ec0d3c1ab841b4981ef8630addb1d1",
            "92d8c6a47090485885cd2dbf5029fbd4",
            "76903af17e4946669ba8055a5983f0dd",
            "fecf369702eb4670b736ac94adca1af0",
            "eca68657f8ba4a06b125e2f8bcaffb98",
            "7f80c7791076475f97ec7ee0c26beac3",
            "5ffdc01a6141482d9ffda74060e77ecf",
            "52221db3e3044b97884c61206087ed17",
            "f05f231d69b841d69d552973d33c84cd",
            "a8fbf50938c04f70809ecfe235601d07",
            "4cf65fbf4020443785e5bf664376be56",
            "ae12a615668d440c9d5e799496b9f3d1"
          ]
        },
        "id": "1AjY8VMuc3QK",
        "outputId": "1133f0d0-6e19-46a9-bec7-e7ba383de3b3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "78486a6464094cce85909900641547cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9428279acdbe4583844e5dc0be8a3919",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e6cc2d9714749ed9053c2c9745498b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/105 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "BertForSequenceClassification LOAD REPORT from: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
            "Key                          | Status     |  | \n",
            "-----------------------------+------------+--+-\n",
            "bert.embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "209aef5c7ab0404187743d7af0901c82",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad333bec169f44a8a8ace28117147915",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e134cf3668974876940fa0a92d2bf51b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a3345449b54a4615a447c8e3ec5945eb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "92d8c6a47090485885cd2dbf5029fbd4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "crossEncoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "pairs = [[question, doc.page_content] for doc in flashDocs]\n",
        "\n",
        "scores = crossEncoder.predict(pairs)\n",
        "crossReranked = list(zip(scores, flashDocs))\n",
        "crossReranked = sorted(crossReranked, reverse=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODgzxLJwSpFe",
        "outputId": "eb8e98e5-fa95-4ea6-a188-00d37bc5c57a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "37\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['What is mentioned about Memory-based Architectures on page 9?',\n",
              " 'distributed representations, which makes the memory both (i) human-readable, lending a form of\\ninterpretability to our model, and (ii) human-writable, enabling us to dynamically update the model’s\\nmemory by editing the document index. This approach has also been used in knowledge-intensive\\ndialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF']"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(len(pairs))\n",
        "print()\n",
        "pairs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtBO8Bvl7eiM",
        "outputId": "3c86b438-15c2-40c2-e598-0dfdfd86a13d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "37\n",
            "3.2602353\n",
            "can be ﬁne-tuned for strong performance on a variety of tasks.\n",
            "Memory-based Architectures Our document index can be seen as a large external memory for\n",
            "neural networks to attend to, analogous to memor\n"
          ]
        }
      ],
      "source": [
        "print(len(crossReranked))\n",
        "print(crossReranked[0][0])   # Relevence score\n",
        "print(crossReranked[0][1].page_content[:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MJiDUpM7mEw",
        "outputId": "b22c8750-111a-4c2c-e1e8-fcc657607916"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5\n",
            "can be ﬁne-tuned for strong performance on a variety of tasks.\n",
            "Memory-based Architectures Our document index can be seen as a large external memory for\n",
            "neural networks to attend to, analogous to memor\n"
          ]
        }
      ],
      "source": [
        "K = 5\n",
        "finalRerankedDocs = [doc for _, doc in crossReranked[:K]]\n",
        "print(len(finalRerankedDocs))\n",
        "print(finalRerankedDocs[0].page_content[:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6MlzQiNmzx6"
      },
      "source": [
        "### STEP 6 (PARENT DOCUMENT RETRIEVER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "XzLJVvw3nEV4"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "def ParentDocumentRetriever(ranked_docs,all_chunks):  # Topk docs and all chunks\n",
        "\n",
        "    page_to_chunks = defaultdict(list)\n",
        "\n",
        "    for chunk in all_chunks:\n",
        "        page = chunk.metadata.get(\"page\")\n",
        "        page_to_chunks[page].append(chunk)\n",
        "\n",
        "    parent_docs = []\n",
        "    seen_pages = set()\n",
        "\n",
        "    for doc in ranked_docs:\n",
        "        page = doc.metadata.get(\"page\")\n",
        "\n",
        "        if page in seen_pages:\n",
        "            continue\n",
        "\n",
        "        seen_pages.add(page)\n",
        "\n",
        "        page_chunks = page_to_chunks.get(page, [])\n",
        "\n",
        "        merged_text = \" \".join(\n",
        "            chunk.page_content for chunk in page_chunks\n",
        "        )\n",
        "\n",
        "        parent_docs.append(\n",
        "            Document(\n",
        "                page_content=merged_text,\n",
        "                metadata=doc.metadata\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return parent_docs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uf9l9XaxpiPj",
        "outputId": "d07c9434-1e5d-4ba1-f73b-78997b5b9398"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-ranked chunk pages:\n",
            "[8, 0, 1, 0, 8]\n",
            "\n",
            "Parent document pages:\n",
            "[8, 0, 1]\n"
          ]
        }
      ],
      "source": [
        "print(\"Top-ranked chunk pages:\")\n",
        "print([doc.metadata[\"page\"] for doc in finalRerankedDocs])\n",
        "\n",
        "parentDocs = ParentDocumentRetriever(finalRerankedDocs, chunks)\n",
        "\n",
        "print(\"\\nParent document pages:\")\n",
        "print([doc.metadata[\"page\"] for doc in parentDocs])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYsGDEFzqiDF",
        "outputId": "32bff583-4764-4d3d-92cb-973b10173821"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(parentDocs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzTw-S_MporE",
        "outputId": "790f0542-b378-4b76-9c60-fbef099a0359"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunk length: 353\n",
            "Parent length: 5637\n"
          ]
        }
      ],
      "source": [
        "print(\"Chunk length:\", len(finalRerankedDocs[0].page_content))\n",
        "print(\"Parent length:\", len(parentDocs[0].page_content))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oek0Z5eupsU9",
        "outputId": "eb221507-200a-4db5-b329-8708794a5ad2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page_content='can be ﬁne-tuned for strong performance on a variety of tasks.\n",
            "Memory-based Architectures Our document index can be seen as a large external memory for\n",
            "neural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns\n",
            "to retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our' metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/nlp.pdf', 'total_pages': 19, 'page': 8, 'page_label': '9'}\n",
            "\n",
            "General-Purpose Architectures for NLP Prior work on general-purpose architectures for NLP\n",
            "tasks has shown great success without the use of retrieval. A single, pre-trained language model\n",
            "has been shown to achieve strong performance on various classiﬁcation tasks in the GLUE bench-\n",
            "marks [60, 61] after ﬁne-tuning [49, 8]. GPT-2 [50] later showed that a single, left-to-right, pre-trained marks [60, 61] after ﬁne-tuning [49, 8]. GPT-2 [50] later showed that a single, left-to-right, pre-trained\n",
            "language model could achieve strong performance across both discriminative and generative tasks.\n",
            "For further improvement, BART [32] and T5 [51, 52] propose a single, pre-trained encoder-decoder\n",
            "model that leverages bi-directional attention to achieve stronger performance on discriminative model that leverages bi-directional attention to achieve stronger performance on discriminative\n",
            "and generative tasks. Our work aims to expand the space of possible tasks with a single, uniﬁed\n",
            "architecture, by learning a retrieval module to augment pre-trained, generative language models.\n",
            "Learned Retrieval There is signiﬁcant work on learning to retrieve documents in information Learned Retrieval There is signiﬁcant work on learning to retrieve documents in information\n",
            "retrieval, more recently with pre-trained, neural language models [ 44, 26] similar to ours. Some\n",
            "work optimizes the retrieval module to aid in a speciﬁc, downstream task such as question answering,\n",
            "using search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our using search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our\n",
            "work. These successes leverage different retrieval-based architectures and optimization techniques to\n",
            "achieve strong performance on a single task, while we show that a single retrieval-based architecture\n",
            "can be ﬁne-tuned for strong performance on a variety of tasks. can be ﬁne-tuned for strong performance on a variety of tasks.\n",
            "Memory-based Architectures Our document index can be seen as a large external memory for\n",
            "neural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns\n",
            "to retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our to retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our\n",
            "work. Other work improves the ability of dialog models to generate factual text by attending over\n",
            "fact embeddings [15, 13]. A key feature of our memory is that it is comprised of raw text rather\n",
            "distributed representations, which makes the memory both (i) human-readable, lending a form of distributed representations, which makes the memory both (i) human-readable, lending a form of\n",
            "interpretability to our model, and (ii) human-writable, enabling us to dynamically update the model’s\n",
            "memory by editing the document index. This approach has also been used in knowledge-intensive\n",
            "dialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF dialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF\n",
            "rather than end-to-end learnt retrieval [9].\n",
            "Retrieve-and-Edit approaches Our method shares some similarities with retrieve-and-edit style\n",
            "approaches, where a similar training input-output pair is retrieved for a given input, and then edited approaches, where a similar training input-output pair is retrieved for a given input, and then edited\n",
            "to provide a ﬁnal output. These approaches have proved successful in a number of domains including\n",
            "Machine Translation [ 18, 22] and Semantic Parsing [21]. Our approach does have several differences, Machine Translation [ 18, 22] and Semantic Parsing [21]. Our approach does have several differences,\n",
            "including less of emphasis on lightly editing a retrieved item, but on aggregating content from several\n",
            "pieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents pieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents\n",
            "rather than related training pairs. This said, RAG techniques may work well in these settings, and\n",
            "could represent promising future work.\n",
            "6 Discussion\n",
            "In this work, we presented hybrid generation models with access to parametric and non-parametric could represent promising future work.\n",
            "6 Discussion\n",
            "In this work, we presented hybrid generation models with access to parametric and non-parametric\n",
            "memory. We showed that our RAG models obtain state of the art results on open-domain QA. We\n",
            "found that people prefer RAG’s generation over purely parametric BART, ﬁnding RAG more factual found that people prefer RAG’s generation over purely parametric BART, ﬁnding RAG more factual\n",
            "and speciﬁc. We conducted an thorough investigation of the learned retrieval component, validating\n",
            "its effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model its effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model\n",
            "without requiring any retraining. In future work, it may be fruitful to investigate if the two components\n",
            "can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some\n",
            "another objective. Our work opens up new research directions on how parametric and non-parametric\n",
            "memories interact and how to most effectively combine them, showing promise in being applied to a\n",
            "wide variety of NLP tasks.\n",
            "9\n"
          ]
        }
      ],
      "source": [
        "print(finalRerankedDocs[0])\n",
        "print()\n",
        "print(parentDocs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU4sVJbD8ZDH"
      },
      "source": [
        "### STEP 7 (CONTEXT COMPRESSION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "_DMG1gV68cxj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "def ContextualCompression(\n",
        "    query,\n",
        "    docs,\n",
        "    embeddings,\n",
        "    similarity_threshold=0.45\n",
        "):\n",
        "    queryEmbedding = embeddings.embed_query(query)\n",
        "    compressedDocs = []\n",
        "\n",
        "    for doc in docs:\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', doc.page_content)\n",
        "        keptSentences = []\n",
        "\n",
        "        for sent in sentences:\n",
        "            sent_embedding = embeddings.embed_query(sent)\n",
        "            similarity = np.dot(queryEmbedding, sent_embedding) / (\n",
        "                np.linalg.norm(queryEmbedding) * np.linalg.norm(sent_embedding)\n",
        "            )\n",
        "\n",
        "            if similarity >= similarity_threshold:\n",
        "                keptSentences.append(sent)\n",
        "\n",
        "        if keptSentences:\n",
        "            compressedDocs.append(\n",
        "                Document(\n",
        "                    page_content=\" \".join(keptSentences),\n",
        "                    metadata=doc.metadata\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            compressedDocs.append(doc)\n",
        "\n",
        "    return compressedDocs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCe0iKPMfBPt",
        "outputId": "237cb0f0-acd5-4d4b-c056-b23dab3ef4ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n",
            "Memory-based Architectures Our document index can be seen as a large external memory for\n",
            "neural networks to attend to, analogous to memory networks [64, 55].\n"
          ]
        }
      ],
      "source": [
        "compressedDocs = ContextualCompression(\n",
        "    query=question,\n",
        "    docs=parentDocs,\n",
        "    embeddings=embedding\n",
        ")\n",
        "\n",
        "\n",
        "print(len(compressedDocs))\n",
        "print(compressedDocs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MbqkSJVVU2C"
      },
      "source": [
        "### STEP 8 (LongContextReorder) For Solving Lost in Middle Phenomenon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "MjLT_MQtGfkR"
      },
      "outputs": [],
      "source": [
        "finalDocs = compressedDocs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtYE3ZNZVF1y",
        "outputId": "104f6ebe-999d-43a3-f663-48895fd464d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1) --> Memory-based Architectures Our document index can be seen as a large external memory for\n",
            "neural networks to attend to, analogous to memory networks [64, 55].\n",
            "(2) --> There has been extensive previous work proposing architectures to enrich systems with non-parametric\n",
            "memory which are trained from scratch for speciﬁc tasks, e.g. memory networks [ 64, 55], stack-\n",
            "augmented networks [25] and memory layers [ 30].\n",
            "(3) --> Retrieval-Augmented Generation for\n",
            "Knowledge-Intensive NLP Tasks\n",
            "Patrick Lewis†‡, Ethan Perez⋆,\n",
            "Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\n",
            "Mike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†\n",
            "†Facebook AI Research;‡University College London;⋆New York University;\n",
            "plewis@fb.com\n",
            "Abstract †Facebook AI Research;‡University College London;⋆New York University;\n",
            "plewis@fb.com\n",
            "Abstract\n",
            "Large pre-trained language models have been shown to store factual knowledge\n",
            "in their parameters, and achieve state-of-the-art results when ﬁne-tuned on down-\n",
            "stream NLP tasks. However, their ability to access and precisely manipulate knowl- stream NLP tasks. However, their ability to access and precisely manipulate knowl-\n",
            "edge is still limited, and hence on knowledge-intensive tasks, their performance\n",
            "lags behind task-speciﬁc architectures. Additionally, providing provenance for their\n",
            "decisions and updating their world knowledge remain open research problems. Pre- decisions and updating their world knowledge remain open research problems. Pre-\n",
            "trained models with a differentiable access mechanism to explicit non-parametric\n",
            "memory have so far been only investigated for extractive downstream tasks. We\n",
            "explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation\n",
            "(RAG) — models which combine pre-trained parametric and non-parametric mem- (RAG) — models which combine pre-trained parametric and non-parametric mem-\n",
            "ory for language generation. We introduce RAG models where the parametric\n",
            "memory is a pre-trained seq2seq model and the non-parametric memory is a dense\n",
            "vector index of Wikipedia, accessed with a pre-trained neural retriever. We com-\n",
            "pare two RAG formulations, one which conditions on the same retrieved passages pare two RAG formulations, one which conditions on the same retrieved passages\n",
            "across the whole generated sequence, and another which can use different passages\n",
            "per token. We ﬁne-tune and evaluate our models on a wide range of knowledge-\n",
            "intensive NLP tasks and set the state of the art on three open domain QA tasks,\n",
            "outperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract outperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract\n",
            "architectures. For language generation tasks, we ﬁnd that RAG models generate\n",
            "more speciﬁc, diverse and factual language than a state-of-the-art parametric-only\n",
            "seq2seq baseline.\n",
            "1 Introduction\n",
            "Pre-trained neural language models have been shown to learn a substantial amount of in-depth knowl- seq2seq baseline.\n",
            "1 Introduction\n",
            "Pre-trained neural language models have been shown to learn a substantial amount of in-depth knowl-\n",
            "edge from data [47]. They can do so without any access to an external memory, as a parameterized\n",
            "implicit knowledge base [51, 52]. While this development is exciting, such models do have down- implicit knowledge base [51, 52]. While this development is exciting, such models do have down-\n",
            "sides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into\n",
            "their predictions, and may produce “hallucinations” [ 38]. Hybrid models that combine parametric\n",
            "memory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these memory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these\n",
            "issues because knowledge can be directly revised and expanded, and accessed knowledge can be\n",
            "inspected and interpreted. REALM [ 20] and ORQA [ 31], two recently introduced models that\n",
            "combine masked language models [8] with a differentiable retriever, have shown promising results, combine masked language models [8] with a differentiable retriever, have shown promising results,\n",
            "arXiv:2005.11401v4  [cs.CL]  12 Apr 2021\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_transformers import LongContextReorder\n",
        "reorder = LongContextReorder()\n",
        "finalDocsReordered = reorder.transform_documents(finalDocs)\n",
        "\n",
        "for i, doc in enumerate(finalDocsReordered):\n",
        "    print(f\"({i+1}) --> {doc.page_content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KQqnM-TXxwT"
      },
      "source": [
        "### STEP 9 (AUGMENTATION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "M1ocMA9GXaRB",
        "outputId": "e6b2fb60-46b3-477e-8d45-b5ec02d96308"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Memory-based Architectures Our document index can be seen as a large external memory for\\nneural networks to attend to, analogous to memory networks [64, 55].\\n\\n---\\n\\nThere has been extensive previous work proposing architectures to enrich systems with non-parametric\\nmemory which are trained from scratch for speciﬁc tasks, e.g. memory networks [ 64, 55], stack-\\naugmented networks [25] and memory layers [ 30].\\n\\n---\\n\\nRetrieval-Augmented Generation for\\nKnowledge-Intensive NLP Tasks\\nPatrick Lewis†‡, Ethan Perez⋆,\\nAleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\\nMike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†\\n†Facebook AI Research;‡University College London;⋆New York University;\\nplewis@fb.com\\nAbstract †Facebook AI Research;‡University College London;⋆New York University;\\nplewis@fb.com\\nAbstract\\nLarge pre-trained language models have been shown to store factual knowledge\\nin their parameters, and achieve state-of-the-art results when ﬁne-tuned on down-\\nstream NLP tasks. However, their ability to access and precisely manipulate knowl- stream NLP tasks. However, their ability to access and precisely manipulate knowl-\\nedge is still limited, and hence on knowledge-intensive tasks, their performance\\nlags behind task-speciﬁc architectures. Additionally, providing provenance for their\\ndecisions and updating their world knowledge remain open research problems. Pre- decisions and updating their world knowledge remain open research problems. Pre-\\ntrained models with a differentiable access mechanism to explicit non-parametric\\nmemory have so far been only investigated for extractive downstream tasks. We\\nexplore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation\\n(RAG) — models which combine pre-trained parametric and non-parametric mem- (RAG) — models which combine pre-trained parametric and non-parametric mem-\\nory for language generation. We introduce RAG models where the parametric\\nmemory is a pre-trained seq2seq model and the non-parametric memory is a dense\\nvector index of Wikipedia, accessed with a pre-trained neural retriever. We com-\\npare two RAG formulations, one which conditions on the same retrieved passages pare two RAG formulations, one which conditions on the same retrieved passages\\nacross the whole generated sequence, and another which can use different passages\\nper token. We ﬁne-tune and evaluate our models on a wide range of knowledge-\\nintensive NLP tasks and set the state of the art on three open domain QA tasks,\\noutperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract outperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract\\narchitectures. For language generation tasks, we ﬁnd that RAG models generate\\nmore speciﬁc, diverse and factual language than a state-of-the-art parametric-only\\nseq2seq baseline.\\n1 Introduction\\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowl- seq2seq baseline.\\n1 Introduction\\nPre-trained neural language models have been shown to learn a substantial amount of in-depth knowl-\\nedge from data [47]. They can do so without any access to an external memory, as a parameterized\\nimplicit knowledge base [51, 52]. While this development is exciting, such models do have down- implicit knowledge base [51, 52]. While this development is exciting, such models do have down-\\nsides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into\\ntheir predictions, and may produce “hallucinations” [ 38]. Hybrid models that combine parametric\\nmemory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these memory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these\\nissues because knowledge can be directly revised and expanded, and accessed knowledge can be\\ninspected and interpreted. REALM [ 20] and ORQA [ 31], two recently introduced models that\\ncombine masked language models [8] with a differentiable retriever, have shown promising results, combine masked language models [8] with a differentiable retriever, have shown promising results,\\narXiv:2005.11401v4  [cs.CL]  12 Apr 2021'"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def buildContext(docs):\n",
        "    return \"\\n\\n---\\n\\n\".join(doc.page_content for doc in docs)\n",
        "context = buildContext(finalDocsReordered)\n",
        "context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP2cCOaOJ7Xx"
      },
      "source": [
        "### STEP 8 (GENERATION)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "6oZWrGUwgf7B"
      },
      "outputs": [],
      "source": [
        "def RetrieveContext(question):\n",
        "    # Component 1: Self-Query Analyzer\n",
        "    semanticQuery, filters = selfQueryAnalyzer(question)\n",
        "\n",
        "    # Component 2: Hybrid Retrieval (small chunks)\n",
        "    hybridResults = hybridRetriever(\n",
        "        query=semanticQuery,\n",
        "        denseRetriever=similarityRetriever,\n",
        "        sparseRetriever=keywordRetriever,\n",
        "        filters=filters\n",
        "    )\n",
        "\n",
        "    # Component 3: Flash Reranking\n",
        "    passages = [{\"id\": i, \"text\": doc.page_content} for i, doc in enumerate(hybridResults)]\n",
        "    rerankingResults = reranking(semanticQuery, passages, \"Medium\")\n",
        "\n",
        "    flashDocs = [hybridResults[item[\"id\"]] for item in rerankingResults]\n",
        "\n",
        "    # Component 4: Cross Encoder Reranking\n",
        "    pairs = [[semanticQuery, doc.page_content] for doc in flashDocs]\n",
        "    scores = crossEncoder.predict(pairs)\n",
        "    crossReranked = sorted(zip(scores, flashDocs), reverse=True)\n",
        "\n",
        "    top_chunks = [doc for _, doc in crossReranked[:5]]\n",
        "\n",
        "    # Component 5: Parent Document Retriever (small → big)\n",
        "    parentDocs = ParentDocumentRetriever(\n",
        "        ranked_docs=top_chunks,\n",
        "        all_chunks=chunks\n",
        "    )\n",
        "\n",
        "    #  Component 6: Contextual Compression (on parents)\n",
        "    compressedDocs = ContextualCompression(\n",
        "        query=semanticQuery,\n",
        "        docs=parentDocs,\n",
        "        embeddings=embedding\n",
        "    )\n",
        "\n",
        "    #  Component 7: Long Context Reorder\n",
        "    finalDocsReordered = reorder.transform_documents(compressedDocs)\n",
        "\n",
        "    # Component 8: Augmentation\n",
        "    context = buildContext(finalDocsReordered)\n",
        "\n",
        "    return context\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "5d_PHnjeYDBa"
      },
      "outputs": [],
      "source": [
        "def generateResponse(question):\n",
        "    context = RetrieveContext(question)\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"\"\"You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "Use ten sentences maximum and keep the answer concise.\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Question: {question}\n",
        "Answer:\n",
        "\"\"\",\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "\n",
        "    chain = prompt | model | parser\n",
        "    return chain.invoke({\"context\": context, \"question\": question})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCEJH2a-YoSR",
        "outputId": "0de11707-8fac-4c98-853f-cb31dfaab998"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your question related to the document: How does RAG differ from standard sequence-to-sequence models?\n",
            "\n",
            "Generating answer...\n",
            "\n",
            "Answer: RAG differs from standard sequence-to-sequence models by using a retrieval-augmented approach, where a dense vector index of Wikipedia is accessed with a pre-trained neural retriever to retrieve text documents and use them as additional context when generating the target sequence.\n",
            "Do you want to ask another question? (yes/no): yes\n",
            "Enter your question related to the document: What does the paper say on page 5 about Jeopardy Question Generation?\n",
            "\n",
            "Generating answer...\n",
            "\n",
            "Answer: I don't know.\n",
            "Do you want to ask another question? (yes/no): yes\n",
            "Enter your question related to the document: What is the role of Wikipedia in RAG experiments?\n",
            "\n",
            "Generating answer...\n",
            "\n",
            "Answer: Wikipedia serves as a single, non-parametric knowledge source for RAG experiments. It is used to provide evidence for claims and test the model's ability to reason over this evidence.\n",
            "Do you want to ask another question? (yes/no): yes\n",
            "Enter your question related to the document: What do the experimental results indicate about RAG performance?\n",
            "\n",
            "Generating answer...\n",
            "\n",
            "Answer: The experimental results indicate that RAG outperforms the state-of-the-art model in generating factual answers, with evaluators finding it more factual in 42.7% of cases. RAG also provides more specific responses by a large margin. These results demonstrate the effectiveness of RAG over a state-of-the-art generation model.\n",
            "Do you want to ask another question? (yes/no): yes\n",
            "Enter your question related to the document: What downsides of retrieval-based approaches are mentioned?\n",
            "\n",
            "Generating answer...\n",
            "\n",
            "Answer: The text doesn't explicitly mention downsides of retrieval-based approaches. However, it does mention that the proposed approach \"does have several differences\" compared to previous work, including \"less of emphasis on lightly editing a retrieved item\" and \"not learning latent retrieval or retrieving evidence documents rather than related training pairs.\"\n",
            "Do you want to ask another question? (yes/no): yes\n",
            "Enter your question related to the document: Who is Virat Kohli?\n",
            "\n",
            "Generating answer...\n",
            "\n",
            "Answer: I don't know. The provided context does not contain information about Virat Kohli.\n",
            "Do you want to ask another question? (yes/no): no\n",
            "\n",
            "Thank you for using the document Q&A system. Goodbye!\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    question = input(\"Enter your question related to the document: \")\n",
        "\n",
        "    if not question.strip():\n",
        "        print(\"Please enter a valid question.\\n\")\n",
        "        continue\n",
        "\n",
        "    print(\"\\nGenerating answer...\\n\")\n",
        "    answer = generateResponse(question)\n",
        "    print(\"Answer:\", answer)\n",
        "\n",
        "    continue_chat = input(\"Do you want to ask another question? (yes/no): \").strip().lower()\n",
        "    if continue_chat not in ['yes', 'y']:\n",
        "        print(\"\\nThank you for using the document Q&A system. Goodbye!\")\n",
        "        break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
