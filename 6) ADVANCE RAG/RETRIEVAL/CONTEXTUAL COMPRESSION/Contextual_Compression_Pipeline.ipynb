{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhXlPLZl0mAw"
      },
      "outputs": [],
      "source": [
        "!pip install flashrank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqZM5P2J3sv0"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-community langchain-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNyYRq_O4RmR"
      },
      "outputs": [],
      "source": [
        "!pip install pypdf tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvattsXe5hBO"
      },
      "outputs": [],
      "source": [
        "%pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30ZCRxAl6jSY"
      },
      "outputs": [],
      "source": [
        "%pip install rank_bm25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K-ambBG-yB4"
      },
      "source": [
        "## STEP 1 (DATA INGESTION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "BmwFTQYL1fIm",
        "outputId": "b88d1838-818c-4f32-d060-2a81cf7a688f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2cd8c3c5-c18f-4084-aabd-c7dca617a307\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2cd8c3c5-c18f-4084-aabd-c7dca617a307\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving nlp.pdf to nlp.pdf\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nYj0oAJM3fOP"
      },
      "outputs": [],
      "source": [
        "filePath = \"/content/nlp.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MAZEiz-335jf"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0l-qRvNH3oUF"
      },
      "outputs": [],
      "source": [
        "def loadDocs(path:str):\n",
        "  loader = PyPDFLoader(path)\n",
        "  docs = loader.load()\n",
        "  return docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dCaO98l4LFV",
        "outputId": "df56fc7b-0c73-4a69-9087-426ec789d7bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "19\n",
            "page_content='Retrieval-Augmented Generation for\n",
            "Knowledge-Intensive NLP Tasks\n",
            "Patrick Lewis†‡, Ethan Perez⋆,\n",
            "Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\n",
            "Mike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†\n",
            "†Facebook AI Research;‡University College London;⋆New York University;\n",
            "plewis@fb.com\n",
            "Abstract\n",
            "Large pre-trained language models have been shown to store factual knowledge\n",
            "in their parameters, and achieve state-of-the-art results when ﬁne-tuned on down-\n",
            "stream NLP tasks. However, their ability to access and precisely manipulate knowl-\n",
            "edge is still limited, and hence on knowledge-intensive tasks, their performance\n",
            "lags behind task-speciﬁc architectures. Additionally, providing provenance for their\n",
            "decisions and updating their world knowledge remain open research problems. Pre-\n",
            "trained models with a differentiable access mechanism to explicit non-parametric\n",
            "memory have so far been only investigated for extractive downstream tasks. We\n",
            "explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation\n",
            "(RAG) — models which combine pre-trained parametric and non-parametric mem-\n",
            "ory for language generation. We introduce RAG models where the parametric\n",
            "memory is a pre-trained seq2seq model and the non-parametric memory is a dense\n",
            "vector index of Wikipedia, accessed with a pre-trained neural retriever. We com-\n",
            "pare two RAG formulations, one which conditions on the same retrieved passages\n",
            "across the whole generated sequence, and another which can use different passages\n",
            "per token. We ﬁne-tune and evaluate our models on a wide range of knowledge-\n",
            "intensive NLP tasks and set the state of the art on three open domain QA tasks,\n",
            "outperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract\n",
            "architectures. For language generation tasks, we ﬁnd that RAG models generate\n",
            "more speciﬁc, diverse and factual language than a state-of-the-art parametric-only\n",
            "seq2seq baseline.\n",
            "1 Introduction\n",
            "Pre-trained neural language models have been shown to learn a substantial amount of in-depth knowl-\n",
            "edge from data [47]. They can do so without any access to an external memory, as a parameterized\n",
            "implicit knowledge base [51, 52]. While this development is exciting, such models do have down-\n",
            "sides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into\n",
            "their predictions, and may produce “hallucinations” [ 38]. Hybrid models that combine parametric\n",
            "memory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these\n",
            "issues because knowledge can be directly revised and expanded, and accessed knowledge can be\n",
            "inspected and interpreted. REALM [ 20] and ORQA [ 31], two recently introduced models that\n",
            "combine masked language models [8] with a differentiable retriever, have shown promising results,\n",
            "arXiv:2005.11401v4  [cs.CL]  12 Apr 2021' metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/nlp.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1'}\n"
          ]
        }
      ],
      "source": [
        "docs = loadDocs(filePath)\n",
        "print(len(docs))\n",
        "print(docs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NC-DK0xR4cyY"
      },
      "outputs": [],
      "source": [
        "def splitDocs(docs):\n",
        "  splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size = 400,\n",
        "      chunk_overlap = 150\n",
        "  )\n",
        "  chunks = splitter.split_documents(docs)\n",
        "  return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZk4u8Lg4uqg",
        "outputId": "60fcd7b5-f076-45c3-c7f8-dc6a90d4e460"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "262\n",
            "page_content='Retrieval-Augmented Generation for\n",
            "Knowledge-Intensive NLP Tasks\n",
            "Patrick Lewis†‡, Ethan Perez⋆,\n",
            "Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\n",
            "Mike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†\n",
            "†Facebook AI Research;‡University College London;⋆New York University;\n",
            "plewis@fb.com\n",
            "Abstract' metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/nlp.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1'}\n"
          ]
        }
      ],
      "source": [
        "chunks = splitDocs(docs)\n",
        "print(len(chunks))\n",
        "print(chunks[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qMdyDfyU4gKO"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings, ChatHuggingFace, HuggingFaceEndpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_UYWfqZ5Pas"
      },
      "outputs": [],
      "source": [
        "# HF API "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478,
          "referenced_widgets": [
            "ce4570ae3d8c44bf9632c39652ce92b6",
            "f3d833f3dd58456d83a73949480d39a1",
            "5a4cfa33da0144a487ff00e4b6059aae",
            "fea67fc9e0bb41f0a9faac3278dfc62a",
            "2d25de94e2de473d82ae326d941c01f0",
            "d0acba16cf53455aaa64ec4649949d9b",
            "37eae2a066774834bb61dbdf4673aa7b",
            "fb92cdf148c043b9861380a3dea86f84",
            "990c5b690f344c27bd0aec4b4213b661",
            "51818d380aca46fa8942918420aeac65",
            "42b91de7573d46d6b04183b9903f3532",
            "01ae978b2b5e443483dc0a57ad7386ad",
            "2ff0f261c7c4443dbf5aa737a58a4e6f",
            "cd5b9b83d34f4275bebfd5fa75d43bc4",
            "f45ccfce3d3d4e6e8ad12162ba609add",
            "55fc4a0c924c4dbd9eb29ad673c1238c",
            "94b34a3aa5d54b62a78c5fffd04fcc99",
            "9558d99686d14545b4d2f625a911301b",
            "2942fdab5ee9435b9ef1c0c5306224dc",
            "8838018300874c99b4e508e9daf697ab",
            "ed469f0342824de3922760475e743e04",
            "1c3aa5d3445d44f0bbee9dbe3637b6da",
            "6285225f44014ffdb5bf1a8e877641f7",
            "99a800726fdc4e35a88989525a9f408f",
            "22c70e40681043c8990bd09e6f9fea92",
            "071eaf38ba254390bfdfa2b2dc361b1b",
            "2194f759cbdb461a8fe3607e3b72bd71",
            "2ec797045b4b49028a23306cdfef19e3",
            "4f05565a030a4cf19bc410e175e0561d",
            "20ff106bb2064f16ae5bce880c3e0827",
            "c236fb4d3e544e08a98c1e09018b3fde",
            "b9d6f048b7ef40efb3b891807c990a77",
            "0926df9036a64e6797b687ce55613da6",
            "00ec659dacd442bfad6b9aef33e0c847",
            "d85f5a6510b049e7bf23f74f1b47f533",
            "d8fc4616ff2849f1a6bf1830ac2f05f3",
            "1d019dbe20ac4a4d8d1633c349077b00",
            "f89f640b2b6a4a2b8c073fb3af54af59",
            "e6e24fb9fa9a4085bc30e60291cf9460",
            "b492c38ea5ab40248274d1838669e690",
            "a8c8ef7f9fbc475d9cd0f08d04113b84",
            "05b0c73c5aa142feb04a8aa3ab97216c",
            "e2b516878b05489d851e037b23fb3e64",
            "6d0bf0d494b1424eb686a335f3db8331",
            "a60a13f5b7b146988905d3b7ee5fd6c9",
            "d9f127ab63d04d4ab5c530ac73c3da02",
            "c34c93096b2b4bf2b0e88446f688ee7c",
            "2d5b220a1b5a46628f9bfbbc023b4eff",
            "e03b741a966344c4a6a2c7e187545c32",
            "915e0f3d0d084a9fafd851b6f7f462f3",
            "da991c1749f6486b8647e9f4c2a0bea2",
            "801dcce2e86a437c97fb80df3715c9f3",
            "c7823fa561874dd7beed8b32b9e3dc91",
            "e54da5754788405b8878b4f955eace39",
            "5c47383bb0a142509063f079206c039b",
            "cb1e80fb26fd4b35bc865ea289d220a2",
            "9274f40427bc4908a5b6b5dfb72f6d74",
            "22a9e53ef9c34cfaa41997e7e1355c7c",
            "3d030c37be094e9f82302f6b10cb2442",
            "4692d903f99f4b4da9ddd06c2178011b",
            "4560d2b5f1fc454ca01e8357bc17bc22",
            "36c492e479624e2fb89797b0ca5ad0da",
            "9ab61d03d3ec4fd3a0289feefbad7804",
            "09586e19a24e49908c544aa608695c4a",
            "55560aac221b4f06a18a632eeb8da701",
            "554fa5ca9add4b5d9618740f2401ffd2",
            "2e397c682f23477aa7e4337986f0e593",
            "8ae6e4dfefab45e19757c677e0d2934b",
            "1c77b7c9027c4d9abce304a5c8fdbcc0",
            "23367b155dfc421080c6053c69249ac8",
            "cafdba69b69e4545bcbc1de8857f71b5",
            "acf566b53ec64bc096792b4b06e45f15",
            "f257abc7a06b4bffa336eea66cdcfb61",
            "f512434b71fb4065aaba8c9a7e9f0cb7",
            "9f5f49f3cd1445439e47c318d986b226",
            "3ae7987172b047e5af655ff0a348083c",
            "c97c7073742144a98cb6cc50668e4dd4",
            "fb4474d3dc554711ba483cc12a6122b0",
            "eee12b6776f3425eacc296d3bf337438",
            "540fc4444cd349b79fa4aedc1f866936",
            "68cce915d555425eb892b29c3fc8c228",
            "db75776165144b3aad178a4529ed1681",
            "bbddba3e33394829a294c5f3c38aef9b",
            "42b36a0f970b4d30a83d1c2a462a3a25",
            "3b1b37ecf4da482da4f12faf5c2817e1",
            "d8425ebfed26493c8bab3c2fb80d71f4",
            "653bd023bd2647cca2a088591a994fd4",
            "1b177e31fb824e0fbd365e93848924e5",
            "d20879c3d7564b7c8458afad33d93e52",
            "75b97a2ecab34075a38d1103a936fe7b",
            "208c10de30054ed8a4463315d963acc9",
            "7333faf25c2a495ab3b37a26335b65a8",
            "3c62657d5b814b74bb06e2768fff0468",
            "6b5d608157a9467dba97a7ffe3290124",
            "4c0d7ec1aec248dfb41385a3233fafeb",
            "2840d6599f904e23b1d58a69b32c7cf9",
            "d9b74ce2e74446659a1e4d618b3793cc",
            "e989191b86f1418e88e425980330b8ce",
            "a3a7d2b8fbb0488b859ecf8eb16a19d6",
            "8ca0338ac61a475eaaa8ed8ff0ad2bf9",
            "93e69ffb33cb496cbcd6290ccf2f79de",
            "dd1050df3d324608bb4946b98475dc89",
            "d980eaeb50c94d71aa029e85c7e16a0e",
            "844bca538a694bc9a517bcad747c84b9",
            "4f847b8e7462407da25a7fa050a3bc11",
            "99c9fffe503c423d968fe6c00ef4ea4d",
            "607e38658b3040ddbd9781b93b783673",
            "c3940791834a4f459d5db5f05393ca77",
            "bc405982082e4a46aa1dac21b4516eea",
            "54b5a6bc757e40fe874d5986a6054f6f",
            "f64314add01a465f95f912ce1f49fab3",
            "93e8074ee25e450eafba39224f1efd7d",
            "81a51bbdfc474e5d8947ecba8f895ff3",
            "1ca2e233f5b0419eaf8f4ce41d63187b",
            "ce8995f35c4e437b8c4bf55aa4aadac8",
            "595817915d2441e3ae707143f3a7ccf8",
            "dd28d1f291bd465d800d7dcb5e2ad2db",
            "0bad18d261b844dc862d76c7fc16c648",
            "a350174f80d34c5d9d1d70c4291b5ef7",
            "ab254bcf26e4412bad3a4967df95b87a",
            "730373a8e2ba45869f94abe7027ae755"
          ]
        },
        "id": "ChqbGWVt5BC6",
        "outputId": "732078cb-0509-4df4-972f-4a22502843bf"
      },
      "outputs": [],
      "source": [
        "embedding = HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "UU7S5k-b60Mb"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace, HuggingFacePipeline\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id = \"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    task = \"text-generation\",\n",
        ")\n",
        "model = ChatHuggingFace(llm = llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ijUsbmxD95ia"
      },
      "outputs": [],
      "source": [
        "parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiPN3YMp--yq"
      },
      "source": [
        "## STEP 2 (HYBRID SEARCH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "oqhFP3DYymhs"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "vectorStore = Chroma.from_documents(chunks, embedding)\n",
        "similarityRetriever = vectorStore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 20})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "MLeX7Jqyyppd"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "keywordRetriever = BM25Retriever.from_documents(chunks)\n",
        "keywordRetriever.k = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ocka7mc16ucT"
      },
      "outputs": [],
      "source": [
        "def hybridRetriever(\n",
        "    query,\n",
        "    denseRetriever,\n",
        "    sparseRetriever,\n",
        "    denseWeight=0.5,\n",
        "    sparseWeight=0.5,\n",
        "    rrf_k=60\n",
        "):\n",
        "    scores = {}\n",
        "    doc_map = {}\n",
        "\n",
        "    # Dense retrieval\n",
        "    denseDocs = denseRetriever.invoke(query)\n",
        "    for rank, doc in enumerate(denseDocs):\n",
        "        key = doc.page_content\n",
        "        doc_map[key] = doc\n",
        "        scores[key] = scores.get(key, 0) + denseWeight / (rank + 1 + rrf_k)\n",
        "\n",
        "    # Sparse retrieval\n",
        "    sparse_docs = sparseRetriever.invoke(query)\n",
        "    for rank, doc in enumerate(sparse_docs):\n",
        "        key = doc.page_content\n",
        "        doc_map[key] = doc\n",
        "        scores[key] = scores.get(key, 0) + sparseWeight / (rank + 1 + rrf_k)\n",
        "\n",
        "    # Sort by final score\n",
        "    ranked_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return [doc_map[content] for content, _ in ranked_docs]  # List[Document]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPFvuTIg8UMc",
        "outputId": "0bcb11b0-cd7c-45b7-84b8-a3e5c7430c1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22\n",
            "page_content='eration. Rather than use questions from standard open-domain QA tasks, which typically consist\n",
            "of short, simple questions, we propose the more demanding task of generating Jeopardy questions.\n",
            "Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity.\n",
            "For example, “The World Cup” is the answer to the question “In 1986 Mexico scored as the ﬁrst' metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/nlp.pdf', 'total_pages': 19, 'page': 4, 'page_label': '5'}\n"
          ]
        }
      ],
      "source": [
        "query = \"What is Jeopardy Question Generation?\"\n",
        "\n",
        "results = hybridRetriever(\n",
        "    query=query,\n",
        "    denseRetriever= similarityRetriever,\n",
        "    sparseRetriever=keywordRetriever,\n",
        "    denseWeight=0.5,\n",
        "    sparseWeight=0.5\n",
        ")\n",
        "print(len(results))\n",
        "print(results[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC4IeChO_E-I"
      },
      "source": [
        "## STEP 3 (FLASH RERANKING)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqgHbk4yRJSj"
      },
      "outputs": [],
      "source": [
        "!pip install flashrank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "TbAHbohsRQWE"
      },
      "outputs": [],
      "source": [
        "from flashrank.Ranker import Ranker, RerankRequest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "NIm0VUbgRTeW"
      },
      "outputs": [],
      "source": [
        "def reranking(query,passages,choice):\n",
        "  if choice == \"Nano\":\n",
        "    ranker = Ranker()\n",
        "  elif choice == \"Small\":\n",
        "    ranker = Ranker(model_name=\"ms-marco-MiniLM-L-12-v2\", cache_dir=\"/opt\")\n",
        "  elif choice == \"Medium\":\n",
        "    ranker = Ranker(model_name=\"rank-T5-flan\", cache_dir=\"/opt\")\n",
        "  elif choice == \"Large\":\n",
        "    ranker = Ranker(model_name=\"ms-marco-MultiBERT-L-12\", cache_dir=\"/opt\")\n",
        "  rerankrequest = RerankRequest(query=query, passages=passages)\n",
        "  results = ranker.rerank(rerankrequest)\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPdo4SdtMZbM",
        "outputId": "1eedb8b0-388f-436e-ca06-ec840eb26906"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': 0,\n",
              " 'text': 'eration. Rather than use questions from standard open-domain QA tasks, which typically consist\\nof short, simple questions, we propose the more demanding task of generating Jeopardy questions.\\nJeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity.\\nFor example, “The World Cup” is the answer to the question “In 1986 Mexico scored as the ﬁrst'}"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hybridResults = hybridRetriever(\n",
        "    query=query,\n",
        "    denseRetriever=similarityRetriever,\n",
        "    sparseRetriever=keywordRetriever\n",
        ")\n",
        "\n",
        "passages = [\n",
        "    {\"id\": i, \"text\": doc.page_content}\n",
        "    for i, doc in enumerate(hybridResults)\n",
        "]\n",
        "passages[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqcCPUePdkvl",
        "outputId": "1109ed98-f17e-4fc0-8447-c6e001568375"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': 16,\n",
              " 'text': 'T5-11B+SSM[52] 36.6 - /60.5 44.7 -\\nOpen\\nBook\\nREALM [20] 40.4 - / - 40.7 46.8\\nDPR [26] 41.5 57.9/ - 41.1 50.6\\nRAG-Token 44.1 55.2/66.1 45.5 50.0\\nRAG-Seq. 44.5 56.8/68.0 45.2 52.2\\nTable 2: Generation and classiﬁcation Test Scores.\\nMS-MARCO SotA is [4], FEVER-3 is [68] and\\nFEVER-2 is [ 57] *Uses gold context/evidence.\\nBest model without gold access underlined.\\nModel Jeopardy MSMARCO FVR3 FVR2',\n",
              " 'score': np.float32(0.56376076)}"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rerankingResults = reranking(query, passages, \"Medium\")\n",
        "rerankingResults[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PwJa5yPSYDf"
      },
      "source": [
        "## STEP 4 (CROSS-ENCODERS) RERANKING AGAIN FOR MORE ACCURATE RESULTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CHF8IJl6uxm",
        "outputId": "070a7057-704e-43ca-9f0c-16ad0282567e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page_content='T5-11B+SSM[52] 36.6 - /60.5 44.7 -\n",
            "Open\n",
            "Book\n",
            "REALM [20] 40.4 - / - 40.7 46.8\n",
            "DPR [26] 41.5 57.9/ - 41.1 50.6\n",
            "RAG-Token 44.1 55.2/66.1 45.5 50.0\n",
            "RAG-Seq. 44.5 56.8/68.0 45.2 52.2\n",
            "Table 2: Generation and classiﬁcation Test Scores.\n",
            "MS-MARCO SotA is [4], FEVER-3 is [68] and\n",
            "FEVER-2 is [ 57] *Uses gold context/evidence.\n",
            "Best model without gold access underlined.\n",
            "Model Jeopardy MSMARCO FVR3 FVR2' metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/nlp.pdf', 'total_pages': 19, 'page': 5, 'page_label': '6'}\n"
          ]
        }
      ],
      "source": [
        "flash_docs = [hybridResults[item[\"id\"]] for item in rerankingResults]\n",
        "print(flash_docs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODgzxLJwSpFe",
        "outputId": "d077f973-b04e-4b58-d471-d1963825a379"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(np.float32(6.8061676),\n",
              " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/nlp.pdf', 'total_pages': 19, 'page': 18, 'page_label': '19'}, page_content='Jeopardy Question Generation 97392 13714 26849\\nMS-MARCO 153726 12468 101093*\\nFEVER-3-way 145450 10000 10000\\nFEVER-2-way 96966 6666 6666\\nparameters. The best performing \"closed-book\" (parametric only) open-domain QA model is T5-11B\\nwith 11 Billion trainable parameters. The T5 model with the closest number of parameters to our'))"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "crossEncoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "\n",
        "pairs = [[query, doc.page_content] for doc in flash_docs]\n",
        "scores = crossEncoder.predict(pairs)\n",
        "\n",
        "crossReranked = list(zip(scores, flash_docs))\n",
        "crossReranked = sorted(crossReranked, reverse=True)\n",
        "crossReranked[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtBO8Bvl7eiM",
        "outputId": "d8d6e3c4-4133-45fd-93d7-cac57c3993fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22\n",
            "6.8061676\n",
            "Jeopardy Question Generation 97392 13714 26849\n",
            "MS-MARCO 153726 12468 101093*\n",
            "FEVER-3-way 145450 10000 10000\n",
            "FEVER-2-way 96966 6666 6666\n",
            "parameters. The best performing \"closed-book\" (parametric only) \n"
          ]
        }
      ],
      "source": [
        "print(len(crossReranked))\n",
        "print(crossReranked[0][0])   # Relevence score\n",
        "print(crossReranked[0][1].page_content[:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MJiDUpM7mEw",
        "outputId": "93fe611c-e566-4a16-86bd-d1ba26b621b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n",
            "Jeopardy Question Generation 97392 13714 26849\n",
            "MS-MARCO 153726 12468 101093*\n",
            "FEVER-3-way 145450 10000 10000\n",
            "FEVER-2-way 96966 6666 6666\n",
            "parameters. The best performing \"closed-book\" (parametric only) \n"
          ]
        }
      ],
      "source": [
        "K = 4\n",
        "finalRerankedDocs = [doc for _, doc in crossReranked[:K]]\n",
        "print(len(finalRerankedDocs))\n",
        "print(finalRerankedDocs[0].page_content[:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU4sVJbD8ZDH"
      },
      "source": [
        "### STEP 5 (CONTEXT COMPRESSION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "_DMG1gV68cxj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "def ContextualCompression(\n",
        "    query,\n",
        "    docs,\n",
        "    embeddings,\n",
        "    similarity_threshold=0.45\n",
        "):\n",
        "    queryEmbedding = embeddings.embed_query(query)\n",
        "    compressedDocs = []\n",
        "\n",
        "    for doc in docs:\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', doc.page_content)\n",
        "        keptSentences = []\n",
        "\n",
        "        for sent in sentences:\n",
        "            sent_embedding = embeddings.embed_query(sent)\n",
        "            similarity = np.dot(queryEmbedding, sent_embedding) / (\n",
        "                np.linalg.norm(queryEmbedding) * np.linalg.norm(sent_embedding)\n",
        "            )\n",
        "\n",
        "            if similarity >= similarity_threshold:\n",
        "                keptSentences.append(sent)\n",
        "\n",
        "        if keptSentences:\n",
        "            compressedDocs.append(\n",
        "                Document(\n",
        "                    page_content=\" \".join(keptSentences),\n",
        "                    metadata=doc.metadata\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            compressedDocs.append(doc)\n",
        "\n",
        "    return compressedDocs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LljSlMMxAyYK",
        "outputId": "37ebc84e-f964-4a5c-a89b-40c27c617a49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n",
            "Jeopardy Question Generation 97392 13714 26849\n",
            "MS-MARCO 153726 12468 101093*\n",
            "FEVER-3-way 145450 10000 10000\n",
            "FEVER-2-way 96966 6666 6666\n",
            "parameters. The best performing \"closed-book\" (parametric only) open-domain QA model is T5-11B\n",
            "with 11 Billion trainable parameters. The T5 model with the closest number of parameters to our\n",
            "\n",
            "Question Answering:\n",
            "Answer GenerationRetriever pη\n",
            "(Non-Parametric)\n",
            "z4\n",
            "z3\n",
            "z2\n",
            "z1\n",
            "d(z)\n",
            "Jeopardy Question\n",
            "Generation:\n",
            "Answer Query\n",
            "Figure 1: Overview of our approach.\n"
          ]
        }
      ],
      "source": [
        "compressedDocs = ContextualCompression(\n",
        "    query=query,\n",
        "    docs=finalRerankedDocs,\n",
        "    embeddings=embedding)\n",
        "print(len(compressedDocs))\n",
        "print(compressedDocs[0].page_content)\n",
        "print()\n",
        "print(compressedDocs[1].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MbqkSJVVU2C"
      },
      "source": [
        "### STEP 6 (LongContextReorder) For Solving Lost in Middle Phenomenon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "MjLT_MQtGfkR"
      },
      "outputs": [],
      "source": [
        "finalDocs = compressedDocs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtYE3ZNZVF1y",
        "outputId": "84c7d53b-5ec6-4313-897e-6bccc3cd1c8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1) --> Question Answering:\n",
            "Answer GenerationRetriever pη\n",
            "(Non-Parametric)\n",
            "z4\n",
            "z3\n",
            "z2\n",
            "z1\n",
            "d(z)\n",
            "Jeopardy Question\n",
            "Generation:\n",
            "Answer Query\n",
            "Figure 1: Overview of our approach.\n",
            "(2) --> 4.3 Jeopardy Question Generation\n",
            "Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation,\n",
            "with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452\n",
            "pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual\n",
            "(3) --> For example, “The World Cup” is the answer to the question “In 1986 Mexico scored as the ﬁrst\n",
            "country to host this international sports competition twice.” As Jeopardy questions are precise,\n",
            "factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a\n",
            "challenging knowledge-intensive generation task.\n",
            "(4) --> Jeopardy Question Generation 97392 13714 26849\n",
            "MS-MARCO 153726 12468 101093*\n",
            "FEVER-3-way 145450 10000 10000\n",
            "FEVER-2-way 96966 6666 6666\n",
            "parameters. The best performing \"closed-book\" (parametric only) open-domain QA model is T5-11B\n",
            "with 11 Billion trainable parameters. The T5 model with the closest number of parameters to our\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_transformers import LongContextReorder\n",
        "reorder = LongContextReorder()\n",
        "finalDocsReordered = reorder.transform_documents(finalDocs)\n",
        "\n",
        "for i, doc in enumerate(finalDocsReordered):\n",
        "    print(f\"({i+1}) --> {doc.page_content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KQqnM-TXxwT"
      },
      "source": [
        "### STEP 7 (AUGMENTATION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "M1ocMA9GXaRB",
        "outputId": "aa673707-2519-4c13-a6e3-04e41bc06db7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Question Answering:\\nAnswer GenerationRetriever pη\\n(Non-Parametric)\\nz4\\nz3\\nz2\\nz1\\nd(z)\\nJeopardy Question\\nGeneration:\\nAnswer Query\\nFigure 1: Overview of our approach.\\n\\n---\\n\\n4.3 Jeopardy Question Generation\\nTable 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation,\\nwith both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452\\npairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual\\n\\n---\\n\\nFor example, “The World Cup” is the answer to the question “In 1986 Mexico scored as the ﬁrst\\ncountry to host this international sports competition twice.” As Jeopardy questions are precise,\\nfactual statements, generating Jeopardy questions conditioned on their answer entities constitutes a\\nchallenging knowledge-intensive generation task.\\n\\n---\\n\\nJeopardy Question Generation 97392 13714 26849\\nMS-MARCO 153726 12468 101093*\\nFEVER-3-way 145450 10000 10000\\nFEVER-2-way 96966 6666 6666\\nparameters. The best performing \"closed-book\" (parametric only) open-domain QA model is T5-11B\\nwith 11 Billion trainable parameters. The T5 model with the closest number of parameters to our'"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def buildContext(docs):\n",
        "    return \"\\n\\n---\\n\\n\".join(doc.page_content for doc in docs)\n",
        "context = buildContext(finalDocsReordered)\n",
        "context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP2cCOaOJ7Xx"
      },
      "source": [
        "### STEP 8 (GENERATION)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "5d_PHnjeYDBa"
      },
      "outputs": [],
      "source": [
        "def generateResponse(question):\n",
        "    \"\"\"\n",
        "    Complete RAG pipeline with hybrid retrieval, flash reranking,\n",
        "    cross-encoder reranking, and long context reordering.\n",
        "    \"\"\"\n",
        "    # Step 1: Hybrid Retrieval\n",
        "    hybridResults = hybridRetriever(\n",
        "        query=question,\n",
        "        denseRetriever=similarityRetriever,\n",
        "        sparseRetriever=keywordRetriever\n",
        "    )\n",
        "\n",
        "    # Step 2: Flash Reranking\n",
        "    passages = [\n",
        "    {\"id\": i, \"text\": doc.page_content}\n",
        "    for i, doc in enumerate(hybridResults)\n",
        "    ]\n",
        "\n",
        "\n",
        "    rerankingResults = reranking(question, passages, \"Medium\")\n",
        "    flashTopK = rerankingResults[:8]\n",
        "\n",
        "    # Step 3: Cross-Encoder Reranking\n",
        "    topk_texts = [item[\"text\"] for item in flashTopK]\n",
        "    pairs = [[question, text] for text in topk_texts]\n",
        "\n",
        "    crossEncoderScores = crossEncoder.predict(pairs)\n",
        "    crossReranked = sorted(zip(crossEncoderScores, topk_texts), reverse=True)\n",
        "\n",
        "    FINAL_K = 4\n",
        "    finalTexts = [text for _, text in crossReranked[:FINAL_K]]\n",
        "\n",
        "    # Step 4: Long Context Reorder\n",
        "    finalDocs = [Document(page_content=text) for text in finalTexts]\n",
        "    finalDocsReordered = reorder.transform_documents(finalDocs)\n",
        "\n",
        "    # Step 5: Build Context\n",
        "    context = buildContext(finalDocsReordered)\n",
        "\n",
        "    # Step 6: Generation\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"\"\"You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "Use ten sentences maximum and keep the answer concise.\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Question: {question}\n",
        "Answer:\n",
        "\"\"\",\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "\n",
        "    chain = prompt | model | parser\n",
        "    return chain.invoke({\"context\": context, \"question\": question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCEJH2a-YoSR",
        "outputId": "1214afef-396b-4a44-918e-e11a8385aa77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your question related to the document: What is NLP and RAG ?\n",
            "\n",
            "Generating answer...\n",
            "\n",
            "Answer: NLP, or Natural Language Processing, is a field in computer science that deals with the interaction between computers and human language. RAG, short for Research on Automatous Generation of Responses to Questions, is a language model that uses machine learning to generate human-like responses to questions based on given texts. It has been shown to be more factual and effective in generating correct answers than a state-of-the-art generation model called BART (Bidirectional Encoder Representations from Transformers) in 42.7% of cases, and in 17.1% of cases, both models outperformed BART in a factuality evaluation by human evaluators. RAG also has the ability to generate correct answers in cases where an extractive model would not, achieving 11.8% accuracy in such scenarios. It has potential applications in various scenarios such as aiding with factual information retrieval and improving the effectiveness of marginalization over documents, making it useful in areas like medical indexing and job performance enhancement. However, as with any language model, there are concerns about its potential for abuse in generating misleading or fake content in news and social media. Its use should be approached with caution due to these biases, but to a lesser extent than GPT-2 (Generate according to: \"source, will probably never be entirely factual and completely devoid of bias.\"\n",
            "Do you want to ask another question? (yes/no): yes\n",
            "Enter your question related to the document: Who is Virat Kohli?\n",
            "\n",
            "Generating answer...\n",
            "\n",
            "Answer: I'm sorry, but the provided context does not mention Virat Kohli. Please provide additional information or a different question. If you're referring to the Indian cricket captain Virat Kohli, our NQ RAG model might not have enough training data to accurately answer your question as it focuses on world leaders and only has an accuracy of 12% for mismatched dates and indices. The context provided is related to a research paper titled \"Retrieval-Augmented Generation (RAG)\" which proposes a method called retrieval-augmented generation (RAG) for question-answering tasks using a pre-trained seq2seq transformer and a dense vector index of Wikipedia accessed with a neural retriever. The authors are listed as Thomas Wolf, Lysandre Debut, Victor Sanh, et al. In the paper \"Retrieval-Augmented Generation for Open-Domain Question Answering (2020). This paper is related to a more general paper titled \"Attention is all you need\" by Vaswani et al. Published in 2017. The paper's abstract and related work section can be found at arXiv.org/abs/1706.0414366.\n",
            "Do you want to ask another question? (yes/no): no\n",
            "\n",
            "Thank you for using the document Q&A system. Goodbye!\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    question = input(\"Enter your question related to the document: \")\n",
        "\n",
        "    if not question.strip():\n",
        "        print(\"Please enter a valid question.\\n\")\n",
        "        continue\n",
        "\n",
        "    print(\"\\nGenerating answer...\\n\")\n",
        "    answer = generateResponse(question)\n",
        "    print(\"Answer:\", answer)\n",
        "\n",
        "    continue_chat = input(\"Do you want to ask another question? (yes/no): \").strip().lower()\n",
        "    if continue_chat not in ['yes', 'y']:\n",
        "        print(\"\\nThank you for using the document Q&A system. Goodbye!\")\n",
        "        break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
