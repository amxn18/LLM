{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhXlPLZl0mAw"
      },
      "outputs": [],
      "source": [
        "%pip install flashrank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqZM5P2J3sv0"
      },
      "outputs": [],
      "source": [
        "%pip install langchain langchain-community langchain-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNyYRq_O4RmR"
      },
      "outputs": [],
      "source": [
        "%pip install pypdf tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvattsXe5hBO"
      },
      "outputs": [],
      "source": [
        "%pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30ZCRxAl6jSY"
      },
      "outputs": [],
      "source": [
        "%pip install rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4p_dMaz5h-Bd"
      },
      "outputs": [],
      "source": [
        "%pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K-ambBG-yB4"
      },
      "source": [
        "## STEP 1 (DATA INGESTION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "BmwFTQYL1fIm",
        "outputId": "97a3578b-27ff-452c-b82f-0b6c4a94a5fc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3c75f66a-e59f-4cd6-8d4f-9dd9955bcba7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3c75f66a-e59f-4cd6-8d4f-9dd9955bcba7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving nlp.pdf to nlp.pdf\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nYj0oAJM3fOP"
      },
      "outputs": [],
      "source": [
        "filePath = \"/content/nlp.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MAZEiz-335jf"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0l-qRvNH3oUF"
      },
      "outputs": [],
      "source": [
        "def loadDocs(path:str):\n",
        "  loader = PyPDFLoader(path)\n",
        "  docs = loader.load()\n",
        "  return docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dCaO98l4LFV",
        "outputId": "53d47a94-2888-4820-ff48-7c4ab62bc929"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "19\n",
            "page_content='Retrieval-Augmented Generation for\n",
            "Knowledge-Intensive NLP Tasks\n",
            "Patrick Lewis†‡, Ethan Perez⋆,\n",
            "Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\n",
            "Mike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†\n",
            "†Facebook AI Research;‡University College London;⋆New York University;\n",
            "plewis@fb.com\n",
            "Abstract\n",
            "Large pre-trained language models have been shown to store factual knowledge\n",
            "in their parameters, and achieve state-of-the-art results when ﬁne-tuned on down-\n",
            "stream NLP tasks. However, their ability to access and precisely manipulate knowl-\n",
            "edge is still limited, and hence on knowledge-intensive tasks, their performance\n",
            "lags behind task-speciﬁc architectures. Additionally, providing provenance for their\n",
            "decisions and updating their world knowledge remain open research problems. Pre-\n",
            "trained models with a differentiable access mechanism to explicit non-parametric\n",
            "memory have so far been only investigated for extractive downstream tasks. We\n",
            "explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation\n",
            "(RAG) — models which combine pre-trained parametric and non-parametric mem-\n",
            "ory for language generation. We introduce RAG models where the parametric\n",
            "memory is a pre-trained seq2seq model and the non-parametric memory is a dense\n",
            "vector index of Wikipedia, accessed with a pre-trained neural retriever. We com-\n",
            "pare two RAG formulations, one which conditions on the same retrieved passages\n",
            "across the whole generated sequence, and another which can use different passages\n",
            "per token. We ﬁne-tune and evaluate our models on a wide range of knowledge-\n",
            "intensive NLP tasks and set the state of the art on three open domain QA tasks,\n",
            "outperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract\n",
            "architectures. For language generation tasks, we ﬁnd that RAG models generate\n",
            "more speciﬁc, diverse and factual language than a state-of-the-art parametric-only\n",
            "seq2seq baseline.\n",
            "1 Introduction\n",
            "Pre-trained neural language models have been shown to learn a substantial amount of in-depth knowl-\n",
            "edge from data [47]. They can do so without any access to an external memory, as a parameterized\n",
            "implicit knowledge base [51, 52]. While this development is exciting, such models do have down-\n",
            "sides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into\n",
            "their predictions, and may produce “hallucinations” [ 38]. Hybrid models that combine parametric\n",
            "memory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these\n",
            "issues because knowledge can be directly revised and expanded, and accessed knowledge can be\n",
            "inspected and interpreted. REALM [ 20] and ORQA [ 31], two recently introduced models that\n",
            "combine masked language models [8] with a differentiable retriever, have shown promising results,\n",
            "arXiv:2005.11401v4  [cs.CL]  12 Apr 2021' metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/nlp.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1'}\n"
          ]
        }
      ],
      "source": [
        "docs = loadDocs(filePath)\n",
        "print(len(docs))\n",
        "print(docs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NC-DK0xR4cyY"
      },
      "outputs": [],
      "source": [
        "def splitDocs(docs):\n",
        "  splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size = 400,\n",
        "      chunk_overlap = 150\n",
        "  )\n",
        "  chunks = splitter.split_documents(docs)\n",
        "  return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZk4u8Lg4uqg",
        "outputId": "97a4193a-3660-462f-fa29-0f8fe3294c25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "262\n",
            "page_content='Retrieval-Augmented Generation for\n",
            "Knowledge-Intensive NLP Tasks\n",
            "Patrick Lewis†‡, Ethan Perez⋆,\n",
            "Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\n",
            "Mike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†\n",
            "†Facebook AI Research;‡University College London;⋆New York University;\n",
            "plewis@fb.com\n",
            "Abstract' metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/nlp.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1'}\n"
          ]
        }
      ],
      "source": [
        "chunks = splitDocs(docs)\n",
        "print(len(chunks))\n",
        "print(chunks[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qMdyDfyU4gKO"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings, ChatHuggingFace, HuggingFaceEndpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_UYWfqZ5Pas"
      },
      "outputs": [],
      "source": [
        "# HF API KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChqbGWVt5BC6"
      },
      "outputs": [],
      "source": [
        "embedding = HuggingFaceEmbeddings(model_name = 'sentence-transformers/all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "UU7S5k-b60Mb"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace, HuggingFacePipeline\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id = \"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    task = \"text-generation\",\n",
        ")\n",
        "model = ChatHuggingFace(llm = llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ijUsbmxD95ia"
      },
      "outputs": [],
      "source": [
        "parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5RNDoLmRgGC"
      },
      "source": [
        "### STEP 2 (SELF QUERYING ANALYZER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qii99YBVRm85"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "import json\n",
        "selfQueryPrompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "You are a query analyzer.\n",
        "\n",
        "Your task is to split the question into:\n",
        "1. semantic_query → what should be searched semantically\n",
        "2. filters → structured metadata constraints\n",
        "\n",
        "Allowed filters:\n",
        "- page (integer)\n",
        "\n",
        "RULES:\n",
        "- If the question mentions \"page X\", extract page = X\n",
        "- Remove filter-related words from the semantic query\n",
        "- If no filters apply, return an empty filters object\n",
        "\n",
        "EXAMPLES:\n",
        "\n",
        "Question:\n",
        "\"What is discussed on page 6 about Jeopardy Question Generation?\"\n",
        "\n",
        "Output:\n",
        "{{\n",
        "  \"semantic_query\": \"Jeopardy Question Generation\",\n",
        "  \"filters\": {{ \"page\": 6 }}\n",
        "}}\n",
        "\n",
        "Question:\n",
        "\"Explain Jeopardy Question Generation\"\n",
        "\n",
        "Output:\n",
        "{{\n",
        "  \"semantic_query\": \"Jeopardy Question Generation\",\n",
        "  \"filters\": {{}}\n",
        "}}\n",
        "\n",
        "NOW ANALYZE THIS QUESTION:\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Return ONLY valid JSON.\n",
        "\"\"\",\n",
        "    input_variables=[\"question\"]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "rkSVlzJbTD39"
      },
      "outputs": [],
      "source": [
        "def selfQueryAnalyzer(question):\n",
        "    chain = selfQueryPrompt | model | parser\n",
        "    response = chain.invoke({\"question\": question})\n",
        "\n",
        "    try:\n",
        "        parsed = json.loads(response)\n",
        "        semanticQuery = parsed.get(\"semantic_query\", question)\n",
        "        filters = parsed.get(\"filters\", {})\n",
        "    except Exception as e:\n",
        "        print(\"Parsing failed:\", e)\n",
        "        semanticQuery = question\n",
        "        filters = {}\n",
        "\n",
        "    return semanticQuery, filters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0P35Zd2ITyjP",
        "outputId": "4bd1fa03-d499-47c9-a9d6-b2fc3530c2dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Semantic Query: Jeopardy Question Generation\n",
            "Filters: {'page': 6}\n"
          ]
        }
      ],
      "source": [
        "q1 = \"What is discussed on page 6 about Jeopardy Question Generation?\"\n",
        "q2 = \"Explain Jeopardy Question Generation\"\n",
        "q3 = \"What are the results in Table 2?\"\n",
        "q4 = \"What are the models discussed on page no 3,4 of this research paper?\"\n",
        "semantic_query, filters = selfQueryAnalyzer(q1)\n",
        "\n",
        "print(\"Semantic Query:\", semantic_query)\n",
        "print(\"Filters:\", filters)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiPN3YMp--yq"
      },
      "source": [
        "## STEP 3 (HYBRID SEARCH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "oqhFP3DYymhs"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "vectorStore = Chroma.from_documents(chunks, embedding)\n",
        "similarityRetriever = vectorStore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 20})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "MLeX7Jqyyppd"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "keywordRetriever = BM25Retriever.from_documents(chunks)\n",
        "keywordRetriever.k = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ocka7mc16ucT"
      },
      "outputs": [],
      "source": [
        "def hybridRetriever(\n",
        "    query,\n",
        "    denseRetriever,\n",
        "    sparseRetriever,\n",
        "    filters=None,\n",
        "    denseWeight=0.5,\n",
        "    sparseWeight=0.5,\n",
        "    rrf_k=60\n",
        "):\n",
        "    scores = {}\n",
        "    doc_map = {}\n",
        "\n",
        "    denseDocs = denseRetriever.invoke(query)\n",
        "    for rank, doc in enumerate(denseDocs):\n",
        "        key = doc.page_content\n",
        "        doc_map[key] = doc\n",
        "        scores[key] = scores.get(key, 0) + denseWeight / (rank + 1 + rrf_k)\n",
        "\n",
        "    sparseDocs = sparseRetriever.invoke(query)\n",
        "    for rank, doc in enumerate(sparseDocs):\n",
        "        key = doc.page_content\n",
        "        doc_map[key] = doc\n",
        "        scores[key] = scores.get(key, 0) + sparseWeight / (rank + 1 + rrf_k)\n",
        "\n",
        "    ranked_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    retrieved_docs = [doc_map[content] for content, _ in ranked_docs]\n",
        "\n",
        "    # Metadata filtering\n",
        "    if filters:\n",
        "        filtered_docs = []\n",
        "        for doc in retrieved_docs:\n",
        "            keep = True\n",
        "            for key, value in filters.items():\n",
        "                if doc.metadata.get(key) != value:\n",
        "                    keep = False\n",
        "                    break\n",
        "            if keep:\n",
        "                filtered_docs.append(doc)\n",
        "\n",
        "        if filtered_docs:\n",
        "            return filtered_docs\n",
        "\n",
        "    return retrieved_docs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jbVeXAXZSbM",
        "outputId": "122bad0f-6e77-4b70-e3d8-481032a1477a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n",
            "page_content='in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\n",
            "4.5 Additional Results\n",
            "Generation Diversity Section 4.3 shows that RAG models are more factual and speciﬁc than\n",
            "BART for Jeopardy question generation. Following recent work on diversity-promoting decoding' metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'keywords': '', 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/nlp.pdf', 'total_pages': 19, 'page': 6, 'page_label': '7'}\n"
          ]
        }
      ],
      "source": [
        "question = \"What is mentioned about Jeopardy Question Generation on page 6?\"\n",
        "semantic_query, filters = selfQueryAnalyzer(question)\n",
        "\n",
        "hybridResults = hybridRetriever(\n",
        "    query=semantic_query,\n",
        "    denseRetriever=similarityRetriever,\n",
        "    sparseRetriever=keywordRetriever,\n",
        "    filters=filters\n",
        ")\n",
        "\n",
        "print(len(hybridResults))\n",
        "print(hybridResults[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC4IeChO_E-I"
      },
      "source": [
        "## STEP 4 (FLASH RERANKING)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqgHbk4yRJSj"
      },
      "outputs": [],
      "source": [
        "!pip install flashrank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "TbAHbohsRQWE"
      },
      "outputs": [],
      "source": [
        "from flashrank.Ranker import Ranker, RerankRequest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "rjp06GhVbXY4"
      },
      "outputs": [],
      "source": [
        "from flashrank.Ranker import Ranker, RerankRequest\n",
        "def reranking(query, passages, choice):\n",
        "    if choice == \"Nano\":\n",
        "        ranker = Ranker()\n",
        "    elif choice == \"Small\":\n",
        "        ranker = Ranker(model_name=\"ms-marco-MiniLM-L-12-v2\", cache_dir=\"/opt\")\n",
        "    elif choice == \"Medium\":\n",
        "        ranker = Ranker(model_name=\"rank-T5-flan\", cache_dir=\"/opt\")\n",
        "    elif choice == \"Large\":\n",
        "        ranker = Ranker(model_name=\"ms-marco-MultiBERT-L-12\", cache_dir=\"/opt\")\n",
        "\n",
        "    rerankRequest = RerankRequest(\n",
        "        query=query,\n",
        "        passages=passages\n",
        "    )\n",
        "\n",
        "    results = ranker.rerank(rerankRequest)\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5aDZdg0bhnp",
        "outputId": "d95f33a0-71ad-470a-93ca-98c343515d0e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': 0,\n",
              " 'text': 'in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\\n4.5 Additional Results\\nGeneration Diversity Section 4.3 shows that RAG models are more factual and speciﬁc than\\nBART for Jeopardy question generation. Following recent work on diversity-promoting decoding'}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "passages = [\n",
        "    {\"id\": i, \"text\": doc.page_content}\n",
        "    for i, doc in enumerate(hybridResults)\n",
        "]\n",
        "passages[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChIouBGJbodo",
        "outputId": "0f3ad8d5-c0bf-44da-e4e7-1b144ca44ccf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "rank-T5-flan.zip: 100%|██████████| 73.7M/73.7M [00:02<00:00, 36.4MiB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'id': 1,\n",
              " 'text': 'ingway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high\\nwhen generating “A Farewell to Arms\" and for document 2 when generating “The Sun Also Rises\".\\nTable 3: Examples from generation tasks. RAG models generate more speciﬁc and factually accurate\\nresponses. ‘?’ indicates factually incorrect responses, * indicates partially correct responses.',\n",
              " 'score': np.float32(0.52198154)}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rerankingResults = reranking(\n",
        "    query=semantic_query,\n",
        "    passages=passages,\n",
        "    choice=\"Medium\"\n",
        ")\n",
        "print(len(rerankingResults))\n",
        "rerankingResults[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PwJa5yPSYDf"
      },
      "source": [
        "## STEP 5 (CROSS-ENCODERS) RERANKING AGAIN FOR MORE ACCURATE RESULTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CHF8IJl6uxm",
        "outputId": "fa5a1d17-57a3-427f-fc10-ca49c0b65d8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page_content='ingway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high\n",
            "when generating “A Farewell to Arms\" and for document 2 when generating “The Sun Also Rises\".\n",
            "Table 3: Examples from generation tasks. RAG models generate more speciﬁc and factually accurate\n",
            "responses. ‘?’ indicates factually incorrect responses, * indicates partially correct responses.' metadata={'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationdate': '2021-04-13T00:48:38+00:00', 'author': '', 'page_label': '7', 'subject': '', 'source': '/content/nlp.pdf', 'total_pages': 19, 'moddate': '2021-04-13T00:48:38+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'keywords': '', 'trapped': '/False', 'page': 6, 'title': ''}\n"
          ]
        }
      ],
      "source": [
        "flashDocs = [hybridResults[item[\"id\"]] for item in rerankingResults]\n",
        "print(flashDocs[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AjY8VMuc3QK"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "crossEncoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "pairs = [[question, doc.page_content] for doc in flashDocs]\n",
        "\n",
        "scores = crossEncoder.predict(pairs)\n",
        "crossReranked = list(zip(scores, flashDocs))\n",
        "crossReranked = sorted(crossReranked, reverse=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODgzxLJwSpFe",
        "outputId": "2ee78a1a-69e9-4dbe-8bfd-894d5b162594"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['What is mentioned about Jeopardy Question Generation on page 6?',\n",
              " 'ingway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high\\nwhen generating “A Farewell to Arms\" and for document 2 when generating “The Sun Also Rises\".\\nTable 3: Examples from generation tasks. RAG models generate more speciﬁc and factually accurate\\nresponses. ‘?’ indicates factually incorrect responses, * indicates partially correct responses.']"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(len(pairs))\n",
        "print()\n",
        "pairs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtBO8Bvl7eiM",
        "outputId": "c333f07b-4274-4a34-ed3a-7ab88e18565e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n",
            "1.7910727\n",
            "in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\n",
            "4.5 Additional Results\n",
            "Generation Diversity Section 4.3 shows that RAG models are more factual and spec\n"
          ]
        }
      ],
      "source": [
        "print(len(crossReranked))\n",
        "print(crossReranked[0][0])   # Relevence score\n",
        "print(crossReranked[0][1].page_content[:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MJiDUpM7mEw",
        "outputId": "ec2c8c9a-9c1c-4f5a-c879-1d724c327049"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n",
            "in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\n",
            "4.5 Additional Results\n",
            "Generation Diversity Section 4.3 shows that RAG models are more factual and spec\n"
          ]
        }
      ],
      "source": [
        "K = 5\n",
        "finalRerankedDocs = [doc for _, doc in crossReranked[:K]]\n",
        "print(len(finalRerankedDocs))\n",
        "print(finalRerankedDocs[0].page_content[:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6MlzQiNmzx6"
      },
      "source": [
        "### STEP 6 (WINDOW SEARCH RETRIEVER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "SZ2JTtOwSqKY"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "def BuildSentenceIndex(allChunks):\n",
        "    pageSentences = defaultdict(list)\n",
        "\n",
        "    for chunk in allChunks:\n",
        "        page = chunk.metadata.get(\"page\")\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', chunk.page_content)\n",
        "\n",
        "        for s in sentences:\n",
        "            clean = s.strip()\n",
        "            if clean:\n",
        "                pageSentences[page].append(clean)\n",
        "\n",
        "    return pageSentences\n",
        "\n",
        "\n",
        "sentenceIndex = BuildSentenceIndex(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "OUEd9y4VStdS"
      },
      "outputs": [],
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "def SentenceWindowRetriever(rankedChunks,sentenceIndex,windowSize=2):\n",
        "    expandedDocs = []\n",
        "\n",
        "    for doc in rankedChunks:\n",
        "        page = doc.metadata.get(\"page\")\n",
        "        if page not in sentenceIndex:\n",
        "            expandedDocs.append(doc)\n",
        "            continue\n",
        "\n",
        "        fullSentences = sentenceIndex[page] # Sentences from entire page not chunk\n",
        "\n",
        "        chunkSentences = re.split(r'(?<=[.!?])\\s+',doc.page_content) # Splitting retreived chunk into sentences\n",
        "\n",
        "        indices = []\n",
        "\n",
        "        # locating sentence positions in full document\n",
        "        for i, sent in enumerate(fullSentences):\n",
        "            for cs in chunkSentences:\n",
        "                if cs.strip() and cs.strip() in sent:\n",
        "                    indices.append(i)\n",
        "\n",
        "        if not indices:\n",
        "            expandedDocs.append(doc)\n",
        "            continue\n",
        "\n",
        "        start = max(0, min(indices) - windowSize)\n",
        "        end = min(len(fullSentences), max(indices) + windowSize + 1)\n",
        "\n",
        "        windowed_sentences = fullSentences[start:end]\n",
        "\n",
        "        expandedDocs.append(\n",
        "            Document(\n",
        "                page_content=\" \".join(dict.fromkeys(windowed_sentences)),\n",
        "                metadata=doc.metadata\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return expandedDocs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIMZ6E1KQN6n",
        "outputId": "993fe8a4-0c09-407b-efc1-b8805ffd2fd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original length: 301\n",
            "Windowed length: 1205\n",
            "\n",
            "ORIGINAL:\n",
            " in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.\n",
            "4.5 Additional Results\n",
            "Generation Diversity Section 4.3 shows that RAG models are more factual and speciﬁc than\n",
            "BART for Jeopardy question generation. Following recent work on diversity-promoting decoding\n",
            "\n",
            "AFTER WINDOWING\n",
            "\n",
            "We also analyze whether documents retrieved by RAG correspond to documents annotated as gold\n",
            "evidence in FEVER. We calculate the overlap in article titles between the topk documents retrieved\n",
            "by RAG and gold evidence annotations. We ﬁnd that the top retrieved document is from a gold article\n",
            "in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases. in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases. 4.5 Additional Results\n",
            "Generation Diversity Section 4.3 shows that RAG models are more factual and speciﬁc than\n",
            "BART for Jeopardy question generation. Following recent work on diversity-promoting decoding BART for Jeopardy question generation. Following recent work on diversity-promoting decoding\n",
            "[33, 59, 39], we also investigate generation diversity by calculating the ratio of distinct ngrams to\n",
            "total ngrams generated by different models. Table 5 shows that RAG-Sequence’s generations are\n",
            "more diverse than RAG-Token’s, and both are signiﬁcantly more diverse than BART without needing more diverse than RAG-Token’s, and both are signiﬁcantly more diverse than BART without needing\n",
            "any diversity-promoting decoding.\n"
          ]
        }
      ],
      "source": [
        "print(\"Original length:\", len(finalRerankedDocs[0].page_content))\n",
        "\n",
        "windowDocs = SentenceWindowRetriever(\n",
        "    ranked_chunks=finalRerankedDocs,\n",
        "    sentence_index=sentenceIndex,\n",
        "    window_size=2\n",
        ")\n",
        "\n",
        "print(\"Windowed length:\", len(windowDocs[0].page_content))\n",
        "\n",
        "print(\"\\nORIGINAL:\\n\", finalRerankedDocs[0].page_content)\n",
        "print()\n",
        "print(\"AFTER WINDOWING\")\n",
        "print()\n",
        "print(windowDocs[0].page_content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0NiOddTYvaT",
        "outputId": "99822adc-e117-4e60-b503-c31465e8a2ab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(windowDocs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KO_9bberXrAV"
      },
      "source": [
        "### STEP 7 (PARENT DOCUMENT RETREIVER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "XzLJVvw3nEV4"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "def ParentDocumentRetriever(ranked_docs,all_chunks):  # Topk docs and all chunks\n",
        "\n",
        "    page_to_chunks = defaultdict(list)\n",
        "\n",
        "    for chunk in all_chunks:\n",
        "        page = chunk.metadata.get(\"page\")\n",
        "        page_to_chunks[page].append(chunk)\n",
        "\n",
        "    parent_docs = []\n",
        "    seen_pages = set()\n",
        "\n",
        "    for doc in ranked_docs:\n",
        "        page = doc.metadata.get(\"page\")\n",
        "\n",
        "        if page in seen_pages:\n",
        "            continue\n",
        "\n",
        "        seen_pages.add(page)\n",
        "\n",
        "        page_chunks = page_to_chunks.get(page, [])\n",
        "\n",
        "        merged_text = \" \".join(\n",
        "            chunk.page_content for chunk in page_chunks\n",
        "        )\n",
        "\n",
        "        parent_docs.append(\n",
        "            Document(\n",
        "                page_content=merged_text,\n",
        "                metadata=doc.metadata\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return parent_docs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uf9l9XaxpiPj",
        "outputId": "b77acc05-460b-412a-c85c-6f84faf0dfbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "Windowed length: 1205\n",
            "Parent length: 5803\n",
            "\n",
            "PARENT DOC SAMPLE:\n",
            "\n",
            "Document 1: his works are considered classics of American\n",
            "literature ... His wartime experiences formed the basis for his novel\n",
            "”A Farewell to Arms” (1929) ...\n",
            "Document 2: ... artists of the 1920s ”Lost Generation” expatriate\n",
            "community . His debut novel, ”The Sun Also Rises” , was published\n",
            "in 1926.\n",
            "BOS\n",
            "”\n",
            "TheSunAlso\n",
            "R ises\n",
            "” is a\n",
            "novel\n",
            "by this\n",
            "author\n",
            "of ” A\n",
            "Farewellto\n",
            "Arms\n",
            "”\n",
            "Doc 1\n",
            "Doc 2\n",
            "Doc 3 in 1926.\n",
            "BOS\n",
            "”\n",
            "TheSunAlso\n",
            "R ises\n",
            "” is a\n",
            "novel\n",
            "by this\n",
            "author\n",
            "of ” A\n",
            "Farewellto\n",
            "Arms\n",
            "”\n",
            "Doc 1\n",
            "Doc 2\n",
            "Doc 3\n",
            "Doc 4\n",
            "Doc 5\n",
            "Figure 2: RAG-Token document posteriorp(zi|x,yi,y−i) for each generated token for input “Hem-\n",
            "ingway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high ingway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high\n",
            "when generating “A Farewell to Arms\" and for document 2 when generating “The Sun Also Rises\".\n",
            "Table 3: Examples from generation tasks. RAG models generate more speciﬁc and factually accurate\n",
            "responses\n"
          ]
        }
      ],
      "source": [
        "parentDocs = ParentDocumentRetriever(\n",
        "    ranked_docs=windowDocs,\n",
        "    all_chunks=chunks\n",
        ")\n",
        "print(len(parentDocs))\n",
        "print(\"Windowed length:\", len(windowDocs[0].page_content))\n",
        "print(\"Parent length:\", len(parentDocs[0].page_content))\n",
        "print()\n",
        "print(\"PARENT DOC SAMPLE:\")\n",
        "print()\n",
        "print(parentDocs[0].page_content[:1000])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fltyJZfnZSYs",
        "outputId": "e1edaf77-57ce-4abe-b909-f98a1c9cb7d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Windowed chunk pages: [6, 6, 6, 6]\n",
            "Parent doc pages: [6]\n"
          ]
        }
      ],
      "source": [
        "print(\"Windowed chunk pages:\", [doc.metadata[\"page\"] for doc in windowDocs])\n",
        "print(\"Parent doc pages:\", [doc.metadata[\"page\"] for doc in parentDocs])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU4sVJbD8ZDH"
      },
      "source": [
        "### STEP 7 (CONTEXT COMPRESSION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "_DMG1gV68cxj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "def ContextualCompression(\n",
        "    query,\n",
        "    docs,\n",
        "    embeddings,\n",
        "    similarity_threshold=0.45\n",
        "):\n",
        "    queryEmbedding = embeddings.embed_query(query)\n",
        "    compressedDocs = []\n",
        "\n",
        "    for doc in docs:\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', doc.page_content)\n",
        "        keptSentences = []\n",
        "\n",
        "        for sent in sentences:\n",
        "            sent_embedding = embeddings.embed_query(sent)\n",
        "            similarity = np.dot(queryEmbedding, sent_embedding) / (\n",
        "                np.linalg.norm(queryEmbedding) * np.linalg.norm(sent_embedding)\n",
        "            )\n",
        "\n",
        "            if similarity >= similarity_threshold:\n",
        "                keptSentences.append(sent)\n",
        "\n",
        "        if keptSentences:\n",
        "            compressedDocs.append(\n",
        "                Document(\n",
        "                    page_content=\" \".join(keptSentences),\n",
        "                    metadata=doc.metadata\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            compressedDocs.append(doc)\n",
        "\n",
        "    return compressedDocs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCe0iKPMfBPt",
        "outputId": "831cf566-1161-4d7a-890f-dd2a11e7ca7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "The posterior for document 1 is high ingway\" for Jeopardy generation with 5 retrieved documents. 4.5 Additional Results\n",
            "Generation Diversity Section 4.3 shows that RAG models are more factual and speciﬁc than\n",
            "BART for Jeopardy question generation. Following recent work on diversity-promoting decoding BART for Jeopardy question generation.\n"
          ]
        }
      ],
      "source": [
        "compressedDocs = ContextualCompression(\n",
        "    query=question,\n",
        "    docs=parentDocs,\n",
        "    embeddings=embedding\n",
        ")\n",
        "\n",
        "\n",
        "print(len(compressedDocs))\n",
        "print(compressedDocs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MbqkSJVVU2C"
      },
      "source": [
        "### STEP 8 (LongContextReorder) For Solving Lost in Middle Phenomenon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "MjLT_MQtGfkR"
      },
      "outputs": [],
      "source": [
        "finalDocs = compressedDocs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtYE3ZNZVF1y",
        "outputId": "8f46788f-018e-48b3-f0f7-90640ce3fcab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1) --> The posterior for document 1 is high ingway\" for Jeopardy generation with 5 retrieved documents. 4.5 Additional Results\n",
            "Generation Diversity Section 4.3 shows that RAG models are more factual and speciﬁc than\n",
            "BART for Jeopardy question generation. Following recent work on diversity-promoting decoding BART for Jeopardy question generation.\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_transformers import LongContextReorder\n",
        "reorder = LongContextReorder()\n",
        "finalDocsReordered = reorder.transform_documents(finalDocs)\n",
        "\n",
        "for i, doc in enumerate(finalDocsReordered):\n",
        "    print(f\"({i+1}) --> {doc.page_content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KQqnM-TXxwT"
      },
      "source": [
        "### STEP 9 (AUGMENTATION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "M1ocMA9GXaRB",
        "outputId": "ef8f2367-3601-4e1a-9f92-0723b9931845"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The posterior for document 1 is high ingway\" for Jeopardy generation with 5 retrieved documents. 4.5 Additional Results\\nGeneration Diversity Section 4.3 shows that RAG models are more factual and speciﬁc than\\nBART for Jeopardy question generation. Following recent work on diversity-promoting decoding BART for Jeopardy question generation.'"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def buildContext(docs):\n",
        "    return \"\\n\\n---\\n\\n\".join(doc.page_content for doc in docs)\n",
        "context = buildContext(finalDocsReordered)\n",
        "context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP2cCOaOJ7Xx"
      },
      "source": [
        "### STEP 8 (GENERATION)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "6oZWrGUwgf7B"
      },
      "outputs": [],
      "source": [
        "def RetrieveContext(question):\n",
        "    # Self-Query Analyzer\n",
        "    semanticQuery, filters = selfQueryAnalyzer(question)\n",
        "\n",
        "    # Hybrid Retrieval (small chunks)\n",
        "    hybridResults = hybridRetriever(\n",
        "        query=semanticQuery,\n",
        "        denseRetriever=similarityRetriever,\n",
        "        sparseRetriever=keywordRetriever,\n",
        "        filters=filters\n",
        "    )\n",
        "\n",
        "    # Flash Reranking\n",
        "    passages = [\n",
        "        {\"id\": i, \"text\": doc.page_content}\n",
        "        for i, doc in enumerate(hybridResults)\n",
        "    ]\n",
        "    rerankingResults = reranking(semanticQuery, passages, \"Medium\")\n",
        "\n",
        "    flashDocs = [hybridResults[item[\"id\"]] for item in rerankingResults]\n",
        "\n",
        "    # Cross-Encoder Reranking\n",
        "    pairs = [[semanticQuery, doc.page_content] for doc in flashDocs]\n",
        "    scores = crossEncoder.predict(pairs)\n",
        "    crossReranked = sorted(zip(scores, flashDocs), reverse=True)\n",
        "\n",
        "    # Top-K most relevant chunks\n",
        "    top_chunks = [doc for _, doc in crossReranked[:5]]\n",
        "\n",
        "    #  Sentence Window Retriever (local context repair)\n",
        "    windowDocs = SentenceWindowRetriever(\n",
        "        rankedChunks=top_chunks,\n",
        "        sentenceIndex=sentenceIndex,\n",
        "        windowSize=2\n",
        "    )\n",
        "\n",
        "    # Parent Document Retriever (small → big)\n",
        "    parentDocs = ParentDocumentRetriever(\n",
        "        ranked_docs=windowDocs,\n",
        "        all_chunks=chunks\n",
        "    )\n",
        "\n",
        "    # Contextual Compression (on parent docs)\n",
        "    compressedDocs = ContextualCompression(\n",
        "        query=semanticQuery,\n",
        "        docs=parentDocs,\n",
        "        embeddings=embedding,\n",
        "        similarity_threshold=0.45\n",
        "    )\n",
        "\n",
        "    # Long Context Reorder\n",
        "    finalDocsReordered = reorder.transform_documents(compressedDocs)\n",
        "\n",
        "    # Augmentation\n",
        "    context = buildContext(finalDocsReordered)\n",
        "\n",
        "    return context\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "5d_PHnjeYDBa"
      },
      "outputs": [],
      "source": [
        "def generateResponse(question):\n",
        "    context = RetrieveContext(question)\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        template=\"\"\"You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "Use ten sentences maximum and keep the answer concise.\n",
        "\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Question: {question}\n",
        "Answer:\n",
        "\"\"\",\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "\n",
        "    chain = prompt | model | parser\n",
        "    return chain.invoke({\"context\": context, \"question\": question})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GflT-Zu4bGyK",
        "outputId": "80ac2179-f6aa-41bb-c38c-c9605d6925a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your question related to the document: What is index hot swapping discussed on page 7,8?\n",
            "\n",
            "Generating answer...\n",
            "\n",
            "Answer: Index hot-swapping is discussed on page 7 and 8 as an advantage of non-parametric memory models like RAG, where knowledge can be easily updated at test time without requiring any retraining.\n",
            "Do you want to ask another question? (yes/no): yes \n",
            "Enter your question related to the document: Who is virat kohli?\n",
            "\n",
            "Generating answer...\n",
            "\n",
            "Answer: I don't know who Siraj Raval is, but I do know who Virat Kohli is. Unfortunately, I don't have any information about Virat Kohli in the provided context.\n",
            "Do you want to ask another question? (yes/no): yes\n",
            "Enter your question related to the document: what is effect of Effect of Retrieving more documents?\n",
            "\n",
            "Generating answer...\n",
            "\n",
            "Answer: Retrieving more documents at test time can improve Open-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved documents.\n",
            "Do you want to ask another question? (yes/no): no\n",
            "\n",
            "Thank you for using the document Q&A system. Goodbye!\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    question = input(\"Enter your question related to the document: \")\n",
        "\n",
        "    if not question.strip():\n",
        "        print(\"Please enter a valid question.\\n\")\n",
        "        continue\n",
        "\n",
        "    print(\"\\nGenerating answer...\\n\")\n",
        "    answer = generateResponse(question)\n",
        "    print(\"Answer:\", answer)\n",
        "\n",
        "    continue_chat = input(\"Do you want to ask another question? (yes/no): \").strip().lower()\n",
        "    if continue_chat not in ['yes', 'y']:\n",
        "        print(\"\\nThank you for using the document Q&A system. Goodbye!\")\n",
        "        break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
